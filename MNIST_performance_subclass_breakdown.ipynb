{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7207c5dd-a90b-475e-a7d2-a0dfa5be31ea",
   "metadata": {},
   "source": [
    "### See how a model performs on the congruent and incongruent test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958ed250-9592-41f9-80e9-c501651ad0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.insert(0, 'src')\n",
    "from utils.visualizations import show_image_rows, make_grid, plot\n",
    "from utils import read_lists\n",
    "\n",
    "import model.metric as module_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063789df-b3c1-4319-932d-840778efdad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = '2_Spurious_MNIST'\n",
    "data_root_dir = os.path.join('data')\n",
    "train_path = os.path.join(data_root_dir, dataset_type, 'training.pt')\n",
    "train_data = torch.load(train_path)\n",
    "\n",
    "test_path = os.path.join(data_root_dir, dataset_type, 'test.pt')\n",
    "test_data = torch.load(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c276c-38a7-4c9d-887f-68ecda7ebb0a",
   "metadata": {},
   "source": [
    "### Load colors and labels for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ac710-f645-40c0-adba-18850b0cfce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_imgs = train_data['images']\n",
    "# train_labels = train_data['labels']\n",
    "\n",
    "# test_imgs = test_data['images']\n",
    "# test_labels = test_data['labels']\n",
    "\n",
    "n_show = 20\n",
    "for idx, data in enumerate([train_data, test_data]):\n",
    "    imgs = data['images']\n",
    "    labels = data['labels']\n",
    "    print(imgs[0].shape)\n",
    "    print(np.amax(imgs[0]))\n",
    "    show_imgs = imgs[:n_show]\n",
    "    show_labels = labels[:n_show]\n",
    "    show_imgs = make_grid(show_imgs, items_per_row=5)\n",
    "    show_labels = make_grid(show_labels, items_per_row=5)\n",
    "    show_image_rows(\n",
    "        images=show_imgs,\n",
    "        image_titles=show_labels,\n",
    "        image_size=(1.5, 1.5),\n",
    "        figure_title='{} {}'.format(dataset_type, 'Train' if idx == 0 else 'Test'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee838b3f-556b-4b39-af9f-11894cacdf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = '2_Spurious_MNIST'\n",
    "\n",
    "data_dir = os.path.join('data', dataset_type)\n",
    "test_data_path = os.path.join(data_dir, 'test.pt')\n",
    "\n",
    "test_data = torch.load(test_data_path)\n",
    "test_labels = np.array(test_data['labels'])\n",
    "test_colors = np.array(test_data['colors'])\n",
    "\n",
    "congruent_idxs_path = os.path.join(data_dir, 'test_congruent_idxs.pt')\n",
    "incongruent_idxs_path = os.path.join(data_dir, 'test_incongruent_idxs.pt')\n",
    "\n",
    "congruent_idxs = torch.load(congruent_idxs_path)\n",
    "incongruent_idxs = torch.load(incongruent_idxs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c959522f-e3dc-45e4-a3a9-2ec01aea2037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd659da-c51f-48de-afde-03f1109463df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_timestamp = '0317_154335'\n",
    "# model_arch = 'VGG_16'\n",
    "# trial_dir = os.path.join('saved', 'edit_{}'.format(dataset_type), '{}-{}'.format(dataset_type, model_arch), trial_timestamp, 'results', 'edit_idx_6')\n",
    "\n",
    "trial_paths_path = 'saved/edit_2_Spurious_MNIST/method_eac/VGG_16-layernum/0320_111512/trial_paths.txt'\n",
    "trial_paths = read_lists(trial_paths_path)\n",
    "trial_dir = os.path.dirname(trial_paths_path)\n",
    "\n",
    "# trial_logits_path = os.path.join(trial_dir, 'log', 'logits.pth')\n",
    "\n",
    "\n",
    "\n",
    "# trial_logits_path = os.path.join(trial_dir, 'models', 'post_edit_logits.pth')\n",
    "\n",
    "# trial_logits = torch.load(trial_logits_path).cpu().numpy()\n",
    "# trial_predictions = np.argmax(trial_logits, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c72f8c2-230b-4680-b68b-aeb282ba8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print test set metrics for overall, congruent, and incongruent test set samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419ea28-5d2a-400f-8b71-26a10f89c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_save_partitioned_results(pre_edit_predictions: np.array,\n",
    "                                       post_edit_predictions: np.array,\n",
    "                                       labels: np.array,\n",
    "                                       row_data: dict,\n",
    "                                       partition_name: str,\n",
    "                                       metric_fns: list,\n",
    "                                       mean_only: bool\n",
    "                                       ):\n",
    "    print(\"Calculating {} test set performance\".format(partition_name))\n",
    "    \n",
    "    metrics['pre'] = module_metric.compute_metrics(\n",
    "        metric_fns=metric_fns,\n",
    "        prediction=pre_edit_predictions,\n",
    "        target=labels,\n",
    "        unique_labels=[l for l in range(10)],\n",
    "        save_mean=True)\n",
    "    \n",
    "    metrics['post'] = module_metric.compute_metrics(\n",
    "        metric_fns=metric_fns,\n",
    "        prediction=post_edit_predictions,\n",
    "        target=labels,\n",
    "        unique_labels=[l for l in range(10)],\n",
    "        save_mean=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for metric_name in metrics['pre'].keys():\n",
    "        if mean_only and not isinstance(metrics['pre'][metric_name], np.float64):\n",
    "            continue\n",
    "        metric_str = \"{}: \".format(metric_name)\n",
    "        # for i in range(n_trials):\n",
    "        for status in ['pre', 'post']:\n",
    "            metric_value = metrics[status][metric_name]\n",
    "            if np.isscalar(metric_value):\n",
    "                metric_str +=\"{:.4f} \".format(metric_value)\n",
    "                row_data['{} {} {}'.format(partition_name, status, metric_name)] = metric_value\n",
    "            else:\n",
    "                metric_str +=\"{} \".format(metric_value)\n",
    "                row_data[metric_name] = metric_value\n",
    "            if status == 'pre':\n",
    "                metric_str += \"-> \"\n",
    "        print(metric_str)\n",
    "    print(\"\")\n",
    "    \n",
    "    return row_data\n",
    "    \n",
    "def print_summary(congruent_idxs: np.array,\n",
    "                  incongruent_idxs: np.array,\n",
    "                  pre_edit_predictions: np.array,\n",
    "                  post_edit_predictions: np.array, \n",
    "                  test_labels: np.array,\n",
    "                  mean_only=True):\n",
    "    n_trials = len(trial_predictions)\n",
    "    row_data = OrderedDict()\n",
    "    \n",
    "    partition_labels = ['congruent', 'incongruent']\n",
    "    metric_names = [\n",
    "        \"accuracy\",\n",
    "        \"per_class_accuracy\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"f1\",\n",
    "        \"predicted_class_distribution\"]\n",
    "    metric_fns = [getattr(module_metric, metric_name) for metric_name in metric_names]\n",
    "\n",
    "    print(\"Overall test set performance\")\n",
    "    \n",
    "    row_data = print_and_save_partitioned_results(\n",
    "        pre_edit_predictions=pre_edit_predictions,\n",
    "        post_edit_predictions=post_edit_predictions,\n",
    "        labels=test_labels,\n",
    "        row_data=row_data,\n",
    "        partition_name='overall',\n",
    "        metric_fns=metric_fns,\n",
    "        mean_only=mean_only)\n",
    "    \n",
    "    # Do the same but for congruent/incongruent subsets\n",
    "    for label_idx, idxs in enumerate([congruent_idxs, incongruent_idxs]):\n",
    "        congruency_str = 'congruent' if label_idx == 0 else 'incongruent'\n",
    "        metrics = []\n",
    "        \n",
    "        partitioned_labels = test_labels[idxs]\n",
    "        \n",
    "        partitioned_pre_edit_predictions = pre_edit_predictions[idxs]\n",
    "        partitioned_post_edit_predictions = post_edit_predictions[idxs]\n",
    "        \n",
    "        row_data = print_and_save_partitioned_results(\n",
    "            pre_edit_predictions=partitioned_pre_edit_predictions,\n",
    "            post_edit_predictions=partitioned_post_edit_predictions,\n",
    "            labels=partitioned_labels,\n",
    "            row_data=row_data,\n",
    "            partition_name=congruency_str,\n",
    "            metric_fns=metric_fns,\n",
    "            mean_only=mean_only)\n",
    "    \n",
    "    return row_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af6ebb-0c04-4e77-bbcb-c1323b67aa71",
   "metadata": {},
   "source": [
    "### Compare pre vs post edit on each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68529a33-8334-4b50-a148-aae1361dd736",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "csv_save_path = os.path.join(trial_dir, 'results.csv')\n",
    "for trial_path in trial_paths:\n",
    "    # Load pre edit logits & get predictions\n",
    "    pre_edit_trial_logits_path = os.path.join(trial_path, 'models', 'pre_edit_logits.pth')\n",
    "    pre_edit_trial_logits = torch.load(pre_edit_trial_logits_path).cpu().numpy()\n",
    "    pre_edit_trial_predictions = np.argmax(pre_edit_trial_logits, axis=1)\n",
    "\n",
    "    # Load post edit logits & get predictions\n",
    "    post_edit_trial_logits_path = os.path.join(trial_path, 'models', 'post_edit_logits.pth')\n",
    "    post_edit_trial_logits = torch.load(post_edit_trial_logits_path).cpu().numpy()\n",
    "    post_edit_trial_predictions = np.argmax(post_edit_trial_logits, axis=1)\n",
    "    \n",
    "    row_data = OrderedDict()\n",
    "    row_data['path'] = trial_path\n",
    "    row_data.update(print_summary(\n",
    "        congruent_idxs=congruent_idxs,\n",
    "        incongruent_idxs=incongruent_idxs,\n",
    "        pre_edit_predictions = pre_edit_trial_predictions,\n",
    "        post_edit_predictions = post_edit_trial_predictions,\n",
    "        test_labels=test_labels))\n",
    "    \n",
    "    df = df.append(pd.Series(row_data, name=trial_path))\n",
    "\n",
    "df.set_index('path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e54b7-8b7c-4efd-bc4d-9a19710a1fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(csv_save_path)\n",
    "print(\"Saved csv to {}\".format(csv_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480732da-ef24-4ec7-8495-b13f00c14328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print layer number vs congruent post accuracy, incongruent post accuracy, and overall post accuracy\n",
    "plot_save_path = os.path.join(trial_dir, 'layer_v_accuracy.pdf')\n",
    "labels = ['congruent post accuracy', 'incongruent post accuracy', 'overall post accuracy']\n",
    "xs = [[i for i in range(1, 13)] for n in range(3)]\n",
    "ys = [df[label] for label in labels]\n",
    "\n",
    "plot(\n",
    "    xs=xs,\n",
    "    ys=ys,\n",
    "    labels=labels,\n",
    "    title='Editing Layer vs Accuracy',\n",
    "    xlabel='Layer Edited',\n",
    "    ylabel='Accuracy on Subset',\n",
    "    save_path=plot_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c9f39-a268-4f2c-adb1-638ff42cabbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "editing",
   "language": "python",
   "name": "editing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
