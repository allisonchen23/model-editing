{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Edits that match post edit class distribution of target class to real edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from datetime import datetime\n",
    "import collections\n",
    "\n",
    "import warnings\n",
    "from argparse import Namespace\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from helpers import classifier_helpers\n",
    "import helpers.data_helpers as dh\n",
    "import helpers.context_helpers as coh\n",
    "import helpers.rewrite_helpers as rh\n",
    "import helpers.vis_helpers as vh\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Local imports\n",
    "# sys.path.insert(0, 'src')\n",
    "# from utils import read_json, read_lists, list_to_dict, ensure_dir, informal_log, write_lists, get_common_dir_path\n",
    "# from utils.df_utils import load_and_preprocess_csv\n",
    "# from utils.visualizations import histogram\n",
    "# from utils.model_utils import prepare_device, quick_predict\n",
    "# from parse_config import ConfigParser\n",
    "# from test import predict, predict_with_bump\n",
    "# import datasets.datasets as module_data\n",
    "# import model.model as module_arch\n",
    "# import model.metric as module_metric\n",
    "# import model.loss as module_loss\n",
    "# from trainer.editor import Editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'ImageNet'\n",
    "LAYERNUM = 12\n",
    "REWRITE_MODE = 'editing'\n",
    "ARCH = 'vgg16'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = classifier_helpers.get_default_paths(DATASET_NAME, arch=ARCH)\n",
    "DATASET_PATH, MODEL_PATH, MODEL_CLASS, ARCH, CD = ret\n",
    "ret = classifier_helpers.load_classifier(MODEL_PATH, MODEL_CLASS, ARCH,\n",
    "                            DATASET_NAME, LAYERNUM) \n",
    "model, context_model, target_model = ret[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = dh.get_vehicles_on_snow_data(DATASET_NAME, CD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({803: 23, 802: 21, 555: 18, 779: 14, 847: 8, 751: 8, 479: 8, 920: 6, 670: 5, 408: 4, 609: 2, 829: 1, 665: 1, 450: 1, 348: 1, 928: 1, 471: 1, 874: 1, 586: 1, 866: 1, 961: 1, 717: 1, 643: 1}) Counter({779: 17, 555: 17, 751: 17, 803: 15, 670: 9, 847: 9, 479: 9, 802: 8, 920: 7, 408: 3, 665: 2, 609: 2, 866: 2, 829: 1, 450: 1, 348: 1, 928: 1, 251: 1, 471: 1, 867: 1, 874: 1, 586: 1, 961: 1, 717: 1, 643: 1})\n",
      "[0, 3, 1, 1, 1, 4, 0, 13, 8, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 9, 1]\n",
      "int64\n",
      "[802 751 803 670 779 251 555 867 866 408 479 920 665 847 928 450 829 961\n",
      " 586 717 471 348 609 874 643]\n",
      "[13  9  8  4  3  1  1  1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0\n",
      "  0]\n"
     ]
    }
   ],
   "source": [
    "# Load post edit counter\n",
    "restore_dir = os.path.join('edited_checkpoints', 'vehicles_on_snow')\n",
    "post_edit_counter = torch.load(os.path.join(restore_dir, 'post_edit_counter.pth'))\n",
    "pre_edit_counter = torch.load(os.path.join(restore_dir, 'pre_edit_counter.pth'))\n",
    "\n",
    "print(pre_edit_counter, post_edit_counter)\n",
    "# Sort classes based on largest change\n",
    "all_classes = list(set(list(pre_edit_counter.keys()) + list(post_edit_counter.keys())))\n",
    "deltas = []\n",
    "for class_idx in all_classes:\n",
    "    pre_edit_count = pre_edit_counter[class_idx] if class_idx in pre_edit_counter else 0\n",
    "    post_edit_count = post_edit_counter[class_idx] if class_idx in post_edit_counter else 0\n",
    "    deltas.append(np.abs(post_edit_count - pre_edit_count))\n",
    "print(deltas)\n",
    "# Sorts in ascending order\n",
    "sorted_idxs = np.argsort(deltas)\n",
    "sorted_idxs = np.flip(sorted_idxs)\n",
    "\n",
    "# Two lists with corresponding classes and amount they changed by\n",
    "deltas_sorted = np.array(deltas)[sorted_idxs]\n",
    "classes_sorted = np.array(all_classes)[sorted_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_bump(bump_amounts,\n",
    "                      data,\n",
    "                      model):\n",
    "    predictions = []\n",
    "    \n",
    "    if not torch.is_tensor(bump_amounts):\n",
    "        bump_amounts = torch.tensor(bump_amounts)\n",
    "        \n",
    "    for c, x in data.items():\n",
    "        with ch.no_grad():\n",
    "            logits = model(x.cuda())\n",
    "            assert logits.shape == bump_amounts.shape\n",
    "            logits += bump_amounts\n",
    "            pred = logits.argmax(axis=1)\n",
    "            predictions.append(pred)\n",
    "        correct = [p for p in pred if p == c]\n",
    "        acc = 100 * len(correct) / len(x)\n",
    "        print(f'Class: {c}/{CD[c]} | Accuracy: {acc:.2f}',) \n",
    "    predictions = ch.cat(predictions)\n",
    "    \n",
    "    return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_bump(pre_edit_counter,\n",
    "               post_edit_counter,\n",
    "               n_target_predictions,\n",
    "               target_class_idx,\n",
    "               # bumps_preds_metrics,\n",
    "               results_save_dir,\n",
    "               # data_loader,\n",
    "               test_data,\n",
    "               model,\n",
    "               loss_fn,\n",
    "               metric_fns,\n",
    "               device,\n",
    "               cushion=5,\n",
    "               n_stop=10,\n",
    "               debug=True):\n",
    "    '''\n",
    "    Given a number of predictions for target class, obtain bump amount to match and save post edit metrics in results_save_dir\n",
    "    \n",
    "    Arg(s):\n",
    "        n_target_predictions : int\n",
    "            Number of predictions to obtain for target class\n",
    "        target_class_idx : int\n",
    "            index of target class\n",
    "        bumps_preds_metrics : dict\n",
    "            saved data from match_bump_edits()\n",
    "        results_save_dir : str\n",
    "            directory to save results to\n",
    "        data_loader : torch.utils.data.DataLoader\n",
    "            validation data loader to obtain metrics for\n",
    "        model : torch.nn.Module\n",
    "            model\n",
    "        loss_fn : module\n",
    "            loss function\n",
    "        metric_fns : list[model.metric modules]\n",
    "            list of metric functions\n",
    "        device : torch.device\n",
    "            GPU device to run model on\n",
    "        cushion : int\n",
    "            how far away cur_n_target_predictions can be from n_target_predictions on either side to break loop (buffer)\n",
    "        n_stop : int\n",
    "            how many iterations when stuck at the same cur_n_target_predictions until to break the loop\n",
    "        debug : bool\n",
    "            control verbosity\n",
    "        \n",
    "            \n",
    "    Returns: \n",
    "        None\n",
    "    '''\n",
    "    # Unpack bumps_preds_metrics\n",
    "    pre_edit_metrics = bumps_preds_metrics['pre_edit_metrics']\n",
    "    target_class_predictions = bumps_preds_metrics['target_class_predictions']\n",
    "    bump_amounts = bumps_preds_metrics['bump_amounts']\n",
    "    \n",
    "    # Obtain number of total predictions and assert n_target_predictions is less\n",
    "    n_predictions_total = np.sum(pre_edit_metrics['predicted_class_distribution'])\n",
    "    assert n_target_predictions <= n_predictions_total, \\\n",
    "        \"n_target_predictions ({}) must be less than total number of data in dataloader ({})\".format(n_target_predictions, n_predictions_total)\n",
    "    \n",
    "    # Find index above and below n_target_predictions\n",
    "    bin_high_idx = -1\n",
    "    for bin_idx, target_class_prediction in enumerate(target_class_predictions):\n",
    "        if target_class_prediction > n_target_predictions:\n",
    "            bin_high_idx = bin_idx\n",
    "            break\n",
    "            \n",
    "    if bin_high_idx == -1: # Past upper end \n",
    "        # n_predictions_upper_bound\n",
    "        bump_amount_upper_bound = bump_amounts[bin_high_idx] * 2\n",
    "        bump_amount_lower_bound = bump_amounts[bin_high_idx]\n",
    "    # Fall into a bin from the histogram (or lower)\n",
    "    else: \n",
    "        n_predictions_upper_bound = target_class_predictions[bin_high_idx]\n",
    "        bump_amount_upper_bound = bump_amounts[bin_high_idx]\n",
    "\n",
    "        # Store lower bounds for bump_amount and n_predictions\n",
    "        bin_low_idx = bin_high_idx - 1\n",
    "        if bin_low_idx > -1: # First bin is already higher than n_target_predictions\n",
    "            n_predictions_lower_bound = target_class_predictions[bin_low_idx]\n",
    "            bump_amount_lower_bound = bump_amounts[bin_low_idx]\n",
    "        else: # n_target_predictions is less than bump amount for the first bin\n",
    "            n_predictions_lower_bound = 0\n",
    "            bump_amount_lower_bound = -10 \n",
    "        \n",
    "    cur_n_target_predictions = 0\n",
    "    if debug:\n",
    "        print(\"target n_predictions: {}\".format(n_target_predictions))\n",
    "        print(\"Initial bounds for bump: ({}, {})\".format(bump_amount_lower_bound, bump_amount_upper_bound))\n",
    "    \n",
    "    # Keep looping while the difference between current n_target_predictions and goal n_target_predictions is too large\n",
    "    while abs(cur_n_target_predictions - n_target_predictions) > cushion:\n",
    "        # Update bump amount\n",
    "        cur_bump_amount = (bump_amount_lower_bound + bump_amount_upper_bound) / 2\n",
    "        # Check before we undergo an infinite loop\n",
    "        if cur_bump_amount == 0:\n",
    "            print(\"cur_bump_amount is 0, exiting loop\")\n",
    "            break\n",
    "            \n",
    "        # predict using logit bump\n",
    "        log = predict_with_bump(\n",
    "                data_loader=data_loader,\n",
    "                model=model,\n",
    "                loss_fn=loss_fn,\n",
    "                metric_fns=metric_fns,\n",
    "                device=device,\n",
    "                target_class_idx=target_class_idx,\n",
    "                bump_amount=cur_bump_amount,\n",
    "                output_save_path=os.path.join(results_save_dir, \"post_edit_logits.pth\"),\n",
    "                log_save_path=os.path.join(results_save_dir, \"post_edit_metrics.pth\"))\n",
    "\n",
    "        # Obtain num. predictions for target class and determine bin idx\n",
    "        post_class_distribution = log['predicted_class_distribution']\n",
    "        cur_n_target_predictions = post_class_distribution[target_class_idx]\n",
    "        if debug:\n",
    "            print(\"cur_bump_amount: {}, cur_n_target_predictions: {}\".format(cur_bump_amount, cur_n_target_predictions))\n",
    "        \n",
    "        # Update bump bounds of binary search\n",
    "        if cur_n_target_predictions > n_target_predictions:\n",
    "            bump_amount_upper_bound = cur_bump_amount\n",
    "            if debug:\n",
    "                print(\"Updated upper bound to {}\".format(bump_amount_upper_bound))\n",
    "        elif cur_n_target_predictions < n_target_predictions:\n",
    "            bump_amount_lower_bound = cur_bump_amount\n",
    "            if debug:\n",
    "                print(\"Updated lower bound to {}\".format(bump_amount_lower_bound))\n",
    "        \n",
    "    if debug:\n",
    "        print(\"final results: bump amount: {} n_target_predictions: {}\".format(cur_bump_amount, cur_n_target_predictions))\n",
    "    # if results_save_dir is not None:\n",
    "    #     torch.save(log, os.path.join(results_save_dir, \"post_edit_metrics.pth\"))\n",
    "        # torch.save(logits, os.path.join(results_save_dir, \"post_edit_logits.pth\"))\n",
    "    return cur_bump_amount, cur_n_target_predictions, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idxs = np.where(deltas_sorted > 1)[0]\n",
    "selected_deltas = deltas_sorted[selected_idxs]\n",
    "selected_classes = classes_sorted[selected_idxs]\n",
    "\n",
    "match_bump("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below was from CINIC-10 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants, paths\n",
    "class_list_path = os.path.join('metadata', 'cinic-10', 'class_names.txt')\n",
    "\n",
    "# config_path = 'configs/copies/edit_experiments/cinic10_imagenet_bump_noise.json'\n",
    "config_path = 'configs/copies/edit_experiments/cinic10_imagenet_bump_corresponding.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config file, models, and dataloader\n",
    "class_list = read_lists(class_list_path)\n",
    "class_idx_dict = list_to_dict(class_list)\n",
    "\n",
    "config_dict = read_json(config_path)\n",
    "config = ConfigParser(config_dict)\n",
    "device, device_ids = prepare_device(config_dict['n_gpu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelWrapperSanturkar(\n",
       "  (model): VGG(\n",
       "    (normalize): InputNormalize()\n",
       "    (layer0): Sequential(\n",
       "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer5): Sequential(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer6): Sequential(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer7): Sequential(\n",
       "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer8): Sequential(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer9): Sequential(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer10): Sequential(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer11): Sequential(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer12): Sequential(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=[1, 1])\n",
       "    (flatten): Flatten()\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (context_model): VGG(\n",
       "    (normalize): InputNormalize()\n",
       "    (layer0): Sequential(\n",
       "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer5): Sequential(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer6): Sequential(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer7): Sequential(\n",
       "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer8): Sequential(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer9): Sequential(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer10): Sequential(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer11): Sequential(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (layer12): Sequential(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=[1, 1])\n",
       "    (flatten): Flatten()\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (target_model): Sequential(\n",
       "    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "layernum = config.config['layernum']\n",
    "model = config.init_obj('arch', module_arch, layernum=layernum, device=device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "# config = ConfigParser(config_dict)\n",
    "data_loader_args = dict(config.config[\"data_loader\"][\"args\"])\n",
    "dataset_args = dict(config[\"dataset_args\"])\n",
    "\n",
    "val_image_paths = read_lists(config_dict['dataset_paths']['valid_images'])\n",
    "val_labels = read_lists(config_dict['dataset_paths']['valid_labels'])\n",
    "val_paths_data_loader = torch.utils.data.DataLoader(\n",
    "    module_data.CINIC10Dataset(\n",
    "        data_dir=\"\",\n",
    "        image_paths=val_image_paths,\n",
    "        labels=val_labels,\n",
    "        return_paths=True,\n",
    "        **dataset_args\n",
    "    ),\n",
    "    **data_loader_args\n",
    ")\n",
    "\n",
    "# Obtain loss function and metric functions\n",
    "loss_fn = getattr(module_loss, config['loss'])\n",
    "metric_fns = [getattr(module_metric, met) for met in config['metrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class_name = 'airplane'\n",
    "target_class_idx = class_idx_dict[target_class_name]\n",
    "n_select = 100\n",
    "timestamp = '0214_112633'\n",
    "bumps_timestamp = '0208_112555'\n",
    "save_timestamp = '0213_143741'\n",
    "\n",
    "root_dir = os.path.join('saved', 'edit', 'trials', 'CINIC10_ImageNet-VGG_16', '{}' + '_{}'.format(n_select), timestamp)\n",
    "csv_path_template = os.path.join(root_dir, 'results_table.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_class_distribution(csv_path, \n",
    "#                             target_class_idx,\n",
    "#                             target_class_name,\n",
    "#                             show=False,\n",
    "#                             histogram_save_path=None,\n",
    "#                             data_save_path=None):\n",
    "    \n",
    "#     df = load_and_preprocess_csv(\n",
    "#         csv_path=csv_path,\n",
    "#         drop_duplicates=['ID']\n",
    "#     )\n",
    "    \n",
    "#     # Obtain number of predictions for target class pre edit\n",
    "#     pre_edit_class_distribution = df['Pre Class Dist'].to_numpy()\n",
    "#     pre_edit_class_distribution = np.stack(pre_edit_class_distribution)\n",
    "#     target_pre_edit_class_predictions = np.mean(pre_edit_class_distribution[:, target_class_idx])\n",
    "    \n",
    "#     # Obtain number of predictions for target class post edit for each trial\n",
    "#     class_distribution = df['Post Class Dist'].to_numpy()\n",
    "#     class_distribution = np.stack(class_distribution, axis=0)\n",
    "#     target_class_distribution = class_distribution[:, target_class_idx]\n",
    "#     # target_class_bins = np.bincount(target_class_distribution)\n",
    "#     if histogram_save_path is None:\n",
    "#         histogram_save_path = os.path.join(os.path.dirname(csv_path), 'graphs', 'summary', 'target_class_distribution.png')\n",
    "#     title = 'Post Edit {} Class Distribution for {} Edits'.format(target_class_name, target_class_name)\n",
    "#     xlabel = 'Num. {} Predictions Post Edit'.format(target_class_name)\n",
    "#     ylabel = 'Num. Edits'\n",
    "    \n",
    "#     bin_values, bins, _ = histogram(\n",
    "#         data=target_class_distribution,\n",
    "#         n_bins=50,\n",
    "#         title=title,\n",
    "#         xlabel=xlabel,\n",
    "#         ylabel=ylabel,\n",
    "#         marker=target_pre_edit_class_predictions,\n",
    "#         show=show,\n",
    "#         save_path=histogram_save_path)\n",
    "    \n",
    "#     bin_tuples = []\n",
    "#     for bin_idx in range(len(bins) - 1):\n",
    "#         bin_tuples.append((bins[bin_idx], bins[bin_idx+1]))\n",
    "\n",
    "#     save_data = {\n",
    "#         \"n_target_predictions\": target_class_distribution,\n",
    "#         \"histogram_bin_values\": bin_values,\n",
    "#         \"histogram_bins\": bins\n",
    "#     }\n",
    "    \n",
    "#     if data_save_path is None:\n",
    "#         data_save_path = os.path.join(os.path.dirname(csv_path), 'target_class_distribution.pth')\n",
    "#         torch.save(save_data, data_save_path)\n",
    "    \n",
    "#     print(\"Saved target class distribution & histogram data to {}\".format(data_save_path))\n",
    "    \n",
    "#     plt.close('all')\n",
    "#     return save_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Loop over all classes and save histograms and distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in class_list:\n",
    "    csv_path = csv_path_template.format(class_name)\n",
    "    save_class_distribution(\n",
    "        csv_path=csv_path,\n",
    "        target_class_idx=class_idx_dict[class_name],\n",
    "        target_class_name=class_name,\n",
    "        show=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# target_class_distribution_path = os.path.join(root_dir, 'target_class_distribution.pth').format(\n",
    "#     target_class_name)\n",
    "# target_class_distribution = torch.load(target_class_distribution_path)\n",
    "# bin_lows = target_class_distribution['histogram_bins']\n",
    "\n",
    "# bump_save_dir = os.path.join(os.path.dirname(os.path.dirname(config.save_dir)), save_timestamp, '{}_{}'.format(target_class_name, n_select))\n",
    "# ensure_dir(bump_save_dir)\n",
    "# # Save a copy of histogram info to save_dir\n",
    "# torch.save(target_class_distribution, os.path.join(bump_save_dir, 'target_class_distribution.pth'))\n",
    "# # Run bump experiments\n",
    "# print(\"Obtaining class distribution for {} from {}\".format(target_class_name, target_class_distribution_path))\n",
    "# print(\"Saving results to {}\".format(bump_save_dir))\n",
    "# match_bump_edits(\n",
    "#     data_loader=val_paths_data_loader,\n",
    "#     model=model,\n",
    "#     loss_fn=loss_fn,\n",
    "#     metric_fns=metric_fns,\n",
    "#     device=device,\n",
    "#     bin_lows=bin_lows,\n",
    "#     target_class_idx=target_class_idx,\n",
    "#     save_dir=bump_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bump_save_dir = os.path.join(os.path.dirname(os.path.dirname(config.save_dir)), save_timestamp, '{}_{}'.format(target_class_name, n_select))\n",
    "# metrics_save_path = os.path.join(bump_save_dir, 'bumps_preds_metrics.pth')\n",
    "\n",
    "# bumped_target_class_dist = torch.load(metrics_save_path)\n",
    "\n",
    "# bumped_hist_data = []\n",
    "\n",
    "\n",
    "# target_class_dist_dict = torch.load(target_class_distribution_path)\n",
    "\n",
    "# for n_target_predictions, bucket_value in zip(\n",
    "#     bumped_target_class_dist['target_class_predictions'], \n",
    "#     target_class_distribution['histogram_bin_values']):\n",
    "#     cur_data = [n_target_predictions for i in range(int(bucket_value))]\n",
    "#     bumped_hist_data += cur_data\n",
    "    \n",
    "# bins = target_class_distribution['histogram_bins']\n",
    "# bin_values = target_class_distribution['histogram_bin_values']\n",
    "\n",
    "# histogram_save_path = os.path.join(\n",
    "#     bump_save_dir, \n",
    "#     'graphs',\n",
    "#     'summary',\n",
    "#     'bumped_target_class_distribution.png')\n",
    "# bump_bin_values, bump_bins, _= histogram(\n",
    "#         data=bumped_hist_data,\n",
    "#         n_bins=bins, #50,\n",
    "#         title='Bumped Post Edit {} Class Distribution to Match {} Edits'.format(target_class_name, target_class_name),\n",
    "#         xlabel='Num. {} Predictions Post Bump'.format(target_class_name),\n",
    "#         ylabel='Num. Edits',\n",
    "#         save_path=histogram_save_path,\n",
    "#         show=True)\n",
    "\n",
    "# assert (bin_values == bump_bin_values).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Bump for a Corresponding Edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_bump(n_target_predictions,\n",
    "               target_class_idx,\n",
    "               bumps_preds_metrics,\n",
    "               results_save_dir,\n",
    "               data_loader,\n",
    "               model,\n",
    "               loss_fn,\n",
    "               metric_fns,\n",
    "               device,\n",
    "               cushion=5,\n",
    "               n_stop=10,\n",
    "               debug=True):\n",
    "    '''\n",
    "    Given a number of predictions for target class, obtain bump amount to match and save post edit metrics in results_save_dir\n",
    "    \n",
    "    Arg(s):\n",
    "        n_target_predictions : int\n",
    "            Number of predictions to obtain for target class\n",
    "        target_class_idx : int\n",
    "            index of target class\n",
    "        bumps_preds_metrics : dict\n",
    "            saved data from match_bump_edits()\n",
    "        results_save_dir : str\n",
    "            directory to save results to\n",
    "        data_loader : torch.utils.data.DataLoader\n",
    "            validation data loader to obtain metrics for\n",
    "        model : torch.nn.Module\n",
    "            model\n",
    "        loss_fn : module\n",
    "            loss function\n",
    "        metric_fns : list[model.metric modules]\n",
    "            list of metric functions\n",
    "        device : torch.device\n",
    "            GPU device to run model on\n",
    "        cushion : int\n",
    "            how far away cur_n_target_predictions can be from n_target_predictions on either side to break loop (buffer)\n",
    "        n_stop : int\n",
    "            how many iterations when stuck at the same cur_n_target_predictions until to break the loop\n",
    "        debug : bool\n",
    "            control verbosity\n",
    "        \n",
    "            \n",
    "    Returns: \n",
    "        None\n",
    "    '''\n",
    "    # Unpack bumps_preds_metrics\n",
    "    pre_edit_metrics = bumps_preds_metrics['pre_edit_metrics']\n",
    "    target_class_predictions = bumps_preds_metrics['target_class_predictions']\n",
    "    bump_amounts = bumps_preds_metrics['bump_amounts']\n",
    "    \n",
    "    # Obtain number of total predictions and assert n_target_predictions is less\n",
    "    n_predictions_total = np.sum(pre_edit_metrics['predicted_class_distribution'])\n",
    "    assert n_target_predictions <= n_predictions_total, \\\n",
    "        \"n_target_predictions ({}) must be less than total number of data in dataloader ({})\".format(n_target_predictions, n_predictions_total)\n",
    "    \n",
    "    # Find index above and below n_target_predictions\n",
    "    bin_high_idx = -1\n",
    "    for bin_idx, target_class_prediction in enumerate(target_class_predictions):\n",
    "        if target_class_prediction > n_target_predictions:\n",
    "            bin_high_idx = bin_idx\n",
    "            break\n",
    "            \n",
    "    if bin_high_idx == -1: # Past upper end \n",
    "        # n_predictions_upper_bound\n",
    "        bump_amount_upper_bound = bump_amounts[bin_high_idx] * 2\n",
    "        bump_amount_lower_bound = bump_amounts[bin_high_idx]\n",
    "    # Fall into a bin from the histogram (or lower)\n",
    "    else: \n",
    "        n_predictions_upper_bound = target_class_predictions[bin_high_idx]\n",
    "        bump_amount_upper_bound = bump_amounts[bin_high_idx]\n",
    "\n",
    "        # Store lower bounds for bump_amount and n_predictions\n",
    "        bin_low_idx = bin_high_idx - 1\n",
    "        if bin_low_idx > -1: # First bin is already higher than n_target_predictions\n",
    "            n_predictions_lower_bound = target_class_predictions[bin_low_idx]\n",
    "            bump_amount_lower_bound = bump_amounts[bin_low_idx]\n",
    "        else: # n_target_predictions is less than bump amount for the first bin\n",
    "            n_predictions_lower_bound = 0\n",
    "            bump_amount_lower_bound = -10 \n",
    "        \n",
    "    cur_n_target_predictions = 0\n",
    "    if debug:\n",
    "        print(\"target n_predictions: {}\".format(n_target_predictions))\n",
    "        print(\"Initial bounds for bump: ({}, {})\".format(bump_amount_lower_bound, bump_amount_upper_bound))\n",
    "    \n",
    "    # Keep looping while the difference between current n_target_predictions and goal n_target_predictions is too large\n",
    "    while abs(cur_n_target_predictions - n_target_predictions) > cushion:\n",
    "        # Update bump amount\n",
    "        cur_bump_amount = (bump_amount_lower_bound + bump_amount_upper_bound) / 2\n",
    "        # Check before we undergo an infinite loop\n",
    "        if cur_bump_amount == 0:\n",
    "            print(\"cur_bump_amount is 0, exiting loop\")\n",
    "            break\n",
    "            \n",
    "        # predict using logit bump\n",
    "        log = predict_with_bump(\n",
    "                data_loader=data_loader,\n",
    "                model=model,\n",
    "                loss_fn=loss_fn,\n",
    "                metric_fns=metric_fns,\n",
    "                device=device,\n",
    "                target_class_idx=target_class_idx,\n",
    "                bump_amount=cur_bump_amount,\n",
    "                output_save_path=os.path.join(results_save_dir, \"post_edit_logits.pth\"),\n",
    "                log_save_path=os.path.join(results_save_dir, \"post_edit_metrics.pth\"))\n",
    "\n",
    "        # Obtain num. predictions for target class and determine bin idx\n",
    "        post_class_distribution = log['predicted_class_distribution']\n",
    "        cur_n_target_predictions = post_class_distribution[target_class_idx]\n",
    "        if debug:\n",
    "            print(\"cur_bump_amount: {}, cur_n_target_predictions: {}\".format(cur_bump_amount, cur_n_target_predictions))\n",
    "        \n",
    "        # Update bump bounds of binary search\n",
    "        if cur_n_target_predictions > n_target_predictions:\n",
    "            bump_amount_upper_bound = cur_bump_amount\n",
    "            if debug:\n",
    "                print(\"Updated upper bound to {}\".format(bump_amount_upper_bound))\n",
    "        elif cur_n_target_predictions < n_target_predictions:\n",
    "            bump_amount_lower_bound = cur_bump_amount\n",
    "            if debug:\n",
    "                print(\"Updated lower bound to {}\".format(bump_amount_lower_bound))\n",
    "        \n",
    "    if debug:\n",
    "        print(\"final results: bump amount: {} n_target_predictions: {}\".format(cur_bump_amount, cur_n_target_predictions))\n",
    "    # if results_save_dir is not None:\n",
    "    #     torch.save(log, os.path.join(results_save_dir, \"post_edit_metrics.pth\"))\n",
    "        # torch.save(logits, os.path.join(results_save_dir, \"post_edit_logits.pth\"))\n",
    "    return cur_bump_amount, cur_n_target_predictions, log\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # testing\n",
    "\n",
    "# bumps_preds_metrics_path = os.path.join(\n",
    "#     \"saved/edit/experiments/bump_edits\",\n",
    "#     \"bumps_preds_metrics\",\n",
    "#     \"{}_{}_bumps_preds_metrics.pth\".format(target_class_name, n_select))\n",
    "# bumps_preds_metrics = torch.load(bumps_preds_metrics_path)\n",
    "\n",
    "# n_target_predictions = 65000\n",
    "# results_save_dir = os.path.join('temp')\n",
    "# match_bump(n_target_predictions=n_target_predictions,\n",
    "#            target_class_idx=target_class_idx,\n",
    "#            bumps_preds_metrics=bumps_preds_metrics,\n",
    "#            results_save_dir=results_save_dir,\n",
    "#            data_loader=val_paths_data_loader,\n",
    "#            model=model,\n",
    "#            loss_fn=loss_fn,\n",
    "#            metric_fns=metric_fns,\n",
    "#            device=device,\n",
    "#            cushion=5,\n",
    "#            debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of trial paths: 158\n",
      "[0217_140839] Creating corresponding logit bump for airplane-train-n02704645_17657/felzenszwalb_gaussian_softmax (153/158)\n",
      "target n_predictions: 8733\n",
      "Initial bounds for bump: (-10, 0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:30<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: -4.9, cur_n_target_predictions: 5558\n",
      "Updated lower bound to -4.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:26<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: -2.35, cur_n_target_predictions: 7231\n",
      "Updated lower bound to -2.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:07<00:00, 39.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: -1.075, cur_n_target_predictions: 8080\n",
      "Updated lower bound to -1.075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:07<00:00, 38.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: -0.4375, cur_n_target_predictions: 8595\n",
      "Updated lower bound to -0.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:07<00:00, 36.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: -0.11875, cur_n_target_predictions: 8906\n",
      "Updated upper bound to -0.11875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:07<00:00, 35.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: -0.278125, cur_n_target_predictions: 8734\n",
      "Updated upper bound to -0.278125\n",
      "final results: bump amount: -0.278125 n_target_predictions: 8734\n",
      "saved/edit/experiments/corresponding_bump_edits/CINIC10_ImageNet-VGG_16/0213_143741/airplane_100/results/airplane-train-n02704645_17657/felzenszwalb_gaussian_softmax\n",
      "[0217_141006] Creating corresponding logit bump for airplane-train-n03595860_736/felzenszwalb_masked_softmax (154/158)\n",
      "target n_predictions: 18439\n",
      "Initial bounds for bump: (3.637978807091713, 3.751665644813329)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:07<00:00, 37.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 3.694822225952521, cur_n_target_predictions: 18554\n",
      "Updated upper bound to 3.694822225952521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:06<00:00, 39.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 3.666400516522117, cur_n_target_predictions: 18395\n",
      "Updated lower bound to 3.666400516522117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:07<00:00, 37.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 3.680611371237319, cur_n_target_predictions: 18475\n",
      "Updated upper bound to 3.680611371237319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:07<00:00, 36.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 3.673505943879718, cur_n_target_predictions: 18431\n",
      "Updated lower bound to 3.673505943879718\n",
      "final results: bump amount: 3.673505943879718 n_target_predictions: 18431\n",
      "saved/edit/experiments/corresponding_bump_edits/CINIC10_ImageNet-VGG_16/0213_143741/airplane_100/results/airplane-train-n03595860_736/felzenszwalb_masked_softmax\n",
      "[0217_141036] Creating corresponding logit bump for airplane-train-n03595860_736/felzenszwalb_gaussian_softmax (155/158)\n",
      "target n_predictions: 17151\n",
      "Initial bounds for bump: (3.2741809263825417, 3.4788172342814505)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:07<00:00, 37.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 3.376499080331996, cur_n_target_predictions: 16785\n",
      "Updated lower bound to 3.376499080331996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:07<00:00, 37.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 3.4276581573067233, cur_n_target_predictions: 17049\n",
      "Updated lower bound to 3.4276581573067233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:07<00:00, 35.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 3.453237695794087, cur_n_target_predictions: 17184\n",
      "Updated upper bound to 3.453237695794087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:08<00:00, 34.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 3.440447926550405, cur_n_target_predictions: 17120\n",
      "Updated lower bound to 3.440447926550405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:06<00:00, 39.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 3.446842811172246, cur_n_target_predictions: 17153\n",
      "Updated upper bound to 3.446842811172246\n",
      "final results: bump amount: 3.446842811172246 n_target_predictions: 17153\n",
      "saved/edit/experiments/corresponding_bump_edits/CINIC10_ImageNet-VGG_16/0213_143741/airplane_100/results/airplane-train-n03595860_736/felzenszwalb_gaussian_softmax\n",
      "[0217_141114] Creating corresponding logit bump for airplane-train-n04160586_8239/felzenszwalb_masked_softmax (156/158)\n",
      "target n_predictions: 25238\n",
      "Initial bounds for bump: (4.604316927725449, 4.7482018317168695)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:06<00:00, 39.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 4.676259379721159, cur_n_target_predictions: 25292\n",
      "Updated upper bound to 4.676259379721159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:06<00:00, 40.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 4.640288153723304, cur_n_target_predictions: 25024\n",
      "Updated lower bound to 4.640288153723304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:07<00:00, 39.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 4.658273766722232, cur_n_target_predictions: 25145\n",
      "Updated lower bound to 4.658273766722232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:07<00:00, 35.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 4.667266573221696, cur_n_target_predictions: 25230\n",
      "Updated lower bound to 4.667266573221696\n",
      "final results: bump amount: 4.667266573221696 n_target_predictions: 25230\n",
      "saved/edit/experiments/corresponding_bump_edits/CINIC10_ImageNet-VGG_16/0213_143741/airplane_100/results/airplane-train-n04160586_8239/felzenszwalb_masked_softmax\n",
      "[0217_141143] Creating corresponding logit bump for airplane-train-n02691156_6453/felzenszwalb_masked_softmax (157/158)\n",
      "target n_predictions: 17487\n",
      "Initial bounds for bump: (3.4788172342814505, 3.637978807091713)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:07<00:00, 37.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 3.5583980206865817, cur_n_target_predictions: 17773\n",
      "Updated upper bound to 3.5583980206865817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:08<00:00, 33.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 3.518607627484016, cur_n_target_predictions: 17533\n",
      "Updated upper bound to 3.518607627484016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:10<00:00, 25.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 3.4987124308827333, cur_n_target_predictions: 17418\n",
      "Updated lower bound to 3.4987124308827333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:10<00:00, 26.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_bump_amount: 3.5086600291833747, cur_n_target_predictions: 17481\n",
      "Updated lower bound to 3.5086600291833747\n",
      "final results: bump amount: 3.5086600291833747 n_target_predictions: 17481\n",
      "saved/edit/experiments/corresponding_bump_edits/CINIC10_ImageNet-VGG_16/0213_143741/airplane_100/results/airplane-train-n02691156_6453/felzenszwalb_masked_softmax\n",
      "[0217_141220] Found corresponding logit bump for airplane-train-n02691156_6453/felzenszwalb_gaussian_softmax in dictionary (158/158)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 274/274 [00:09<00:00, 28.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved/edit/experiments/corresponding_bump_edits/CINIC10_ImageNet-VGG_16/0213_143741/airplane_100/results/airplane-train-n02691156_6453/felzenszwalb_gaussian_softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_edits = 5\n",
    "cushion = 10\n",
    "original_trials_trial_paths_path = os.path.join(root_dir.format(target_class_name), 'trial_paths.txt')\n",
    "original_trial_paths = read_lists(original_trials_trial_paths_path)\n",
    "common_path = get_common_dir_path(original_trial_paths)\n",
    "print(\"Length of trial paths: {}\".format(len(original_trial_paths)))\n",
    "\n",
    "# Data structure to store how much to bump for each n_predictions in target class\n",
    "bump_amount_dictionary_path = os.path.join(\n",
    "    'metadata',\n",
    "    config_dict['name'], # CINIC10_ImageNet-VGG16\n",
    "    'bump_amounts',\n",
    "    '{}_{}'.format(target_class_name, n_select),\n",
    "    'logit_bump_buffer_{}_dict.pth'.format(cushion))\n",
    "\n",
    "if os.path.isfile(bump_amount_dictionary_path):\n",
    "    bump_amount_dictionary = torch.load(bump_amount_dictionary_path)\n",
    "else:\n",
    "    bump_amount_dictionary = {}\n",
    "ensure_dir(os.path.dirname(bump_amount_dictionary_path))\n",
    "\n",
    "    \n",
    "# Store histogram information\n",
    "bumps_preds_metrics_path = os.path.join(\n",
    "    \"saved/edit/experiments/bump_edits\",\n",
    "    \"bumps_preds_metrics\",\n",
    "    \"{}_{}_bumps_preds_metrics.pth\".format(target_class_name, n_select))\n",
    "bumps_preds_metrics = torch.load(bumps_preds_metrics_path)\n",
    "\n",
    "# Create directories and paths\n",
    "result_root = os.path.join(\n",
    "        os.path.dirname(os.path.dirname(config.save_dir)), \n",
    "        save_timestamp, \n",
    "        '{}_{}'.format(target_class_name, n_select))\n",
    "ensure_dir(result_root)\n",
    "progress_report_path = os.path.join(result_root, 'progress_reports.txt')\n",
    "# Create file to store paths\n",
    "logit_bump_trial_paths_path = os.path.join(result_root, 'trial_paths.txt')\n",
    "if os.path.isfile(logit_bump_trial_paths_path):\n",
    "    os.remove(logit_bump_trial_paths_path)\n",
    "\n",
    "# Iterate through all trial paths\n",
    "for trial_idx, trial_path in enumerate(original_trial_paths):\n",
    "    if trial_idx <= 151:\n",
    "        continue\n",
    "    # Obtain trial ID and create save directory for logit bump results\n",
    "    trial_id = trial_path[len(common_path)+1:]\n",
    "    logit_bump_trial_save_dir = os.path.join(\n",
    "        result_root,\n",
    "        'results',\n",
    "        trial_id,\n",
    "        'models')\n",
    "    ensure_dir(logit_bump_trial_save_dir)\n",
    "    \n",
    "    # Obtain desired n_target_predictions\n",
    "    trial_post_edit_metrics_path = os.path.join(\n",
    "        trial_path,\n",
    "        'models',\n",
    "        'post_edit_metrics.pth')\n",
    "    trial_post_edit_metrics = torch.load(trial_post_edit_metrics_path)\n",
    "    n_target_predictions = trial_post_edit_metrics['predicted_class_distribution'][target_class_idx]\n",
    "    # If bump amount is in dictionary, obtain that value\n",
    "    if n_target_predictions in bump_amount_dictionary:\n",
    "        \n",
    "        informal_log(\"[{}] Found corresponding logit bump for {} in dictionary ({}/{})\".format(\n",
    "            datetime.now().strftime(r'%m%d_%H%M%S'),\n",
    "            trial_id,\n",
    "            trial_idx + 1,\n",
    "            len(original_trial_paths)), progress_report_path)\n",
    "        bump_amount = bump_amount_dictionary[n_target_predictions]\n",
    "        \n",
    "        log = predict_with_bump(\n",
    "            data_loader=val_paths_data_loader,\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            metric_fns=metric_fns,\n",
    "            device=device,\n",
    "            target_class_idx=target_class_idx,\n",
    "            bump_amount=bump_amount,\n",
    "            output_save_path=os.path.join(logit_bump_trial_save_dir, \"post_edit_logits.pth\"),\n",
    "            log_save_path=os.path.join(logit_bump_trial_save_dir, \"post_edit_metrics.pth\"))\n",
    "        # Sanity check that the n_target_predictions with bump is within buffer of target\n",
    "        bump_n_target_predictions = log['predicted_class_distribution'][target_class_idx]\n",
    "        assert abs(n_target_predictions - bump_n_target_predictions) <= cushion\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        informal_log(\"[{}] Creating corresponding logit bump for {} ({}/{})\".format(\n",
    "            datetime.now().strftime(r'%m%d_%H%M%S'),\n",
    "            trial_id,\n",
    "            trial_idx + 1,\n",
    "            len(original_trial_paths)), progress_report_path)\n",
    "        # Find corresponding bump amount\n",
    "        bump_amount, n_target_predictions, metrics = \\\n",
    "            match_bump(\n",
    "                n_target_predictions=n_target_predictions,\n",
    "                target_class_idx=target_class_idx,\n",
    "                bumps_preds_metrics=bumps_preds_metrics,\n",
    "                results_save_dir=logit_bump_trial_save_dir,\n",
    "                data_loader=val_paths_data_loader,\n",
    "                model=model,\n",
    "                loss_fn=loss_fn,\n",
    "                metric_fns=metric_fns,\n",
    "                device=device,\n",
    "                cushion=cushion,\n",
    "                debug=True)\n",
    "        bump_amount_dictionary[n_target_predictions] = bump_amount\n",
    "\n",
    "    # Write trial path to list\n",
    "    informal_log(os.path.dirname(logit_bump_trial_save_dir), logit_bump_trial_paths_path)\n",
    "\n",
    "    # Update the bump dictionary on file\n",
    "    if trial_idx % 10 == 0:\n",
    "        torch.save(bump_amount_dictionary, bump_amount_dictionary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_dir(os.path.dirname(bump_amount_dictionary_path))\n",
    "torch.save(bump_amount_dictionary, bump_amount_dictionary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate bump results on a coarse bin level (repeat bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "editing",
   "language": "python",
   "name": "editing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
