{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Template to Quickly Test Things Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "sys.path.insert(0, 'src')\n",
    "from utils import read_json, read_lists, write_lists, prepare_device\n",
    "from parse_config import ConfigParser\n",
    "from data_loader import data_loaders\n",
    "import model.model as module_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants, paths\n",
    "config_path = 'configs/cinic10_imagenet_misc.json'\n",
    "class_list_path = 'metadata/cinic-10/class_names.txt'\n",
    "target_class =  5  # worst accuracy (40.914%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in config file from configs/cinic10_imagenet_misc.json\n",
      "Initialized model from external_code/PyTorch_CIFAR10/cifar10_models/state_dicts/vgg16_bn.pt\n"
     ]
    }
   ],
   "source": [
    "# Load config file, models\n",
    "config_json = read_json(config_path)\n",
    "config = ConfigParser(config_json)\n",
    "\n",
    "layernum = config.config['layernum']\n",
    "device, device_ids = prepare_device(config['n_gpu'])\n",
    "print(\"Read in config file from {}\".format(config_path))\n",
    "      \n",
    "model = config.init_obj('arch', module_arch, layernum=layernum)\n",
    "model.eval()\n",
    "print(\"Initialized model from {}\".format(config.config['arch']['args']['checkpoint_path']))\n",
    "\n",
    "class_list = read_lists(class_list_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized train data loader\n"
     ]
    }
   ],
   "source": [
    "train_data_loader = data_loader_args = dict(config_json[\"data_loader\"][\"args\"])\n",
    "train_data_loader = data_loaders.CINIC10DataLoader(\n",
    "    **data_loader_args,\n",
    "    return_paths=True,\n",
    "    split='train')\n",
    "\n",
    "print(\"Initialized train data loader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████▌                    | 137/274 [00:32<00:10, 13.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape torch.Size([72, 3, 32, 32])\n",
      "output shape torch.Size([72, 10])\n",
      "torch.Size([19])\n",
      "torch.Size([53])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████████████████████                    | 141/274 [00:33<00:11, 12.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([107])\n",
      "torch.Size([149])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([145])\n",
      "torch.Size([111])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([177])\n",
      "torch.Size([79])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████████████████████▌                   | 144/274 [00:33<00:08, 14.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([172])\n",
      "torch.Size([84])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([121])\n",
      "torch.Size([135])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([150])\n",
      "torch.Size([106])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████████████████████▊                   | 146/274 [00:33<00:10, 11.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([130])\n",
      "torch.Size([126])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([113])\n",
      "torch.Size([143])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([125])\n",
      "torch.Size([131])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|██████████████████████▌                  | 151/274 [00:33<00:07, 15.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([140])\n",
      "torch.Size([116])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([122])\n",
      "torch.Size([134])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([83])\n",
      "torch.Size([173])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([141])\n",
      "torch.Size([115])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([135])\n",
      "torch.Size([121])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|███████████████████████▏                 | 155/274 [00:34<00:08, 14.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([161])\n",
      "torch.Size([95])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([149])\n",
      "torch.Size([107])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([153])\n",
      "torch.Size([103])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████▉                 | 160/274 [00:34<00:06, 18.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([123])\n",
      "torch.Size([133])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([145])\n",
      "torch.Size([111])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([58])\n",
      "torch.Size([198])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([31])\n",
      "torch.Size([225])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([35])\n",
      "torch.Size([221])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████▍                | 163/274 [00:34<00:07, 14.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([27])\n",
      "torch.Size([229])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([27])\n",
      "torch.Size([229])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n",
      "torch.Size([23])\n",
      "torch.Size([233])\n",
      "image shape torch.Size([256, 3, 32, 32])\n",
      "output shape torch.Size([256, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████▋                | 165/274 [00:34<00:08, 13.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35])\n",
      "torch.Size([221])\n",
      "image shape torch.Size([16, 3, 32, 32])\n",
      "output shape torch.Size([16, 10])\n",
      "torch.Size([0])\n",
      "torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 274/274 [00:43<00:00,  6.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Pedal to the metal!\n",
    "correct_image_paths = []\n",
    "correct_images = []\n",
    "incorrect_image_paths = []\n",
    "incorrect_images = []\n",
    "incorrect_predictions = []\n",
    "with torch.no_grad():\n",
    "    for idx, item in enumerate(tqdm(train_data_loader)):\n",
    "        image, target, path = item\n",
    "        \n",
    "        # Skip any batches with no examples from target class\n",
    "        if (target != target_class).all():\n",
    "            continue\n",
    "\n",
    "        # Find indices where target = target class\n",
    "        target_idxs = (target == target_class).nonzero()\n",
    "        target_idxs = torch.squeeze(target_idxs)\n",
    "\n",
    "        image = image[target_idxs]\n",
    "        target = target[target_idxs]\n",
    "        path = [path[idx] for idx in target_idxs]  # path[target_idxs]\n",
    "        \n",
    "        # Move data and label to GPU\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        \n",
    "        print(\"image shape {}\".format(image.shape))\n",
    "        output = model(image)\n",
    "        prediction = torch.argmax(output, dim=1)\n",
    "        print(\"output shape {}\".format(output.shape))\n",
    "        # Obtain indices of where model predicted correctly and incorrectly\n",
    "        correct_idxs = torch.squeeze((prediction == target_class).nonzero())\n",
    "        incorrect_idxs = torch.squeeze((prediction != target_class).nonzero())\n",
    "        \n",
    "        print(correct_idxs.shape)\n",
    "        print(incorrect_idxs.shape)\n",
    "        correct_image_paths += [path[idx] for idx in correct_idxs] \n",
    "        correct_images.append(image[correct_idxs])\n",
    "        \n",
    "        incorrect_image_paths += [path[idx] for idx in incorrect_idxs]\n",
    "        incorrect_images.append(image[incorrect_idxs])\n",
    "        incorrect_predictions.append(prediction[incorrect_idxs])\n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4025\n",
      "2975\n",
      "torch.Size([2975, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# print(len(incorrect_image_paths))\n",
    "# print(len(correct_image_paths))\n",
    "correct_images = torch.cat(correct_images, dim=0)\n",
    "correct_images = correct_images.cpu()\n",
    "# print(correct_images.shape)\n",
    "incorrect_images = torch.cat(incorrect_images, dim=0)\n",
    "incorrect_images = incorrect_images.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save images as checkpoints and paths as txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save paths to correct and incorrect images\n",
    "save_dir = os.path.join(\n",
    "    'metadata', \n",
    "    'CINIC10-ImageNet', \n",
    "    class_list[target_class],\n",
    "    config.config['arch']['args']['type'])\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Save list of correct image paths and the images\n",
    "correct_image_paths_filepath = os.path.join(save_dir, 'correct_image_paths.txt')\n",
    "correct_images_save_path = os.path.join(save_dir, 'correct_images.pth')\n",
    "write_lists(correct_image_paths_filepath, correct_image_paths)\n",
    "torch.save(correct_images, correct_images_save_path)\n",
    "\n",
    "incorrect_image_paths_filepath = os.path.join(save_dir, 'incorrect_image_paths.txt')\n",
    "incorrect_images_save_path = os.path.join(save_dir, 'incorrect_images.pth')\n",
    "write_lists(incorrect_image_paths_filepath, incorrect_image_paths)\n",
    "torch.save(incorrect_images, incorrect_images_save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "editing",
   "language": "python",
   "name": "editing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
