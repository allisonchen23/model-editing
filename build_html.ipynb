{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build HTML file to visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from airium import Airium\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "sys.path.insert(0, 'src')\n",
    "from utils import read_json, read_lists, ensure_dir\n",
    "from utils.df_utils import load_and_preprocess_csv, get_sorted_idxs\n",
    "from utils.html_utils import save_visualizations_separately, build_html\n",
    "from utils.visualizations import bar_graph\n",
    "from parse_config import ConfigParser\n",
    "from data_loader import data_loaders\n",
    "import model.model as module_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "results_timestamp = '0125_134842'\n",
    "target_class = 'dog'\n",
    "n_select = 50\n",
    "paths_timestamp = '0125_112421'\n",
    "data_type = 'softmax'\n",
    "sort_columns = ['Post Target Recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID Regex\n",
    "id_regex = '/+[a-z0-9_]*\\-[a-z0-9_]*\\-[a-z0-9_]*/.*/'\n",
    "def get_image_id(path):\n",
    "    return re.search(id_regex, path).group()[1:-1]\n",
    "\n",
    "# Constant paths\n",
    "class_list_path = os.path.join('metadata', 'cinic-10', 'class_names.txt')\n",
    "# Results paths\n",
    "results_dir = os.path.join('saved', 'edit', 'trials', 'CINIC10_ImageNet-VGG_16', '{}_{}'.format(target_class, n_select), results_timestamp)\n",
    "csv_path = os.path.join(results_dir, 'results_table.csv')\n",
    "trial_paths_path = os.path.join(results_dir, 'trial_paths.txt')\n",
    "\n",
    "paths_dir = os.path.join('paths', 'edits', 'semantics', \n",
    "                         '{}_{}'.format(target_class, n_select), \n",
    "                         paths_timestamp)\n",
    "value_image_paths_path = os.path.join(paths_dir, 'value_images_{}.txt'.format(data_type))\n",
    "\n",
    "# HTML file directories\n",
    "html_save_dir = os.path.join('html', '{}_{}'.format(target_class, n_select))\n",
    "html_assets_dir = os.path.join(html_save_dir, 'assets')\n",
    "ensure_dir(html_assets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load class list\n",
    "class_list = read_lists(class_list_path)\n",
    "# Load CSV and paths\n",
    "df = load_and_preprocess_csv(\n",
    "    csv_path,\n",
    "    drop_duplicates=['ID'])\n",
    "\n",
    "value_image_paths = read_lists(value_image_paths_path)\n",
    "trial_paths = read_lists(trial_paths_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check same number of rows\n",
    "n_rows = len(df)\n",
    "assert len(value_image_paths) == n_rows, \"{} rows in paths; {} rows in data frame\".format(len(value_image_paths), n_rows)\n",
    "assert len(trial_paths) == n_rows, \"{} rows in paths; {} rows in data frame\".format(len(trial_paths), n_rows)\n",
    "\n",
    "# Sanity check that each row corresponds to one another\n",
    "for image_id, value_image_path, trial_path in zip(df['ID'], value_image_paths, trial_paths):\n",
    "    image_id = image_id.split('/')\n",
    "    for id_part in image_id:\n",
    "        assert id_part in value_image_path\n",
    "        assert id_part in trial_path\n",
    "\n",
    "# Check columns in sort_columns are in dataframe\n",
    "for column in sort_columns:\n",
    "    assert column in df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sorted idxs based on sort columns\n",
    "sorted_df, sorted_idxs = get_sorted_idxs(\n",
    "    df=df,\n",
    "    columns=sort_columns,\n",
    "    increasing=False)\n",
    "\n",
    "# Sort image paths and trial paths accordingly\n",
    "sorted_value_image_paths = [value_image_paths[idx] for idx in sorted_idxs]\n",
    "sorted_trial_paths = [trial_paths[idx] for idx in sorted_idxs]\n",
    "sorted_IDs = [re.search(id_regex, path).group()[1:-1] for path in sorted_value_image_paths]\n",
    "\n",
    "# Sanity check\n",
    "for id_, trial_path in zip(sorted_IDs, sorted_trial_paths):\n",
    "    assert id_ in trial_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save metrics as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics for each data point as string\n",
    "sorted_df = sorted_df.round(3)\n",
    "metrics = ['Recall', 'Precision', 'F1']\n",
    "groups = ['Mean', 'Target', 'Orig Pred']\n",
    "metric_strings = []\n",
    "key = '{} {} {}'\n",
    "for idx in range(n_rows):\n",
    "    metric_string = ['ID: {}'.format(sorted_df['ID'].iloc[idx])]\n",
    "    accuracy_key = '{} Accuracy'\n",
    "    metric_string.append(\"Accuracy: {} -> {}\".format(sorted_df[accuracy_key.format('Pre')].iloc[idx], sorted_df[accuracy_key.format('Post')].iloc[idx]))\n",
    "\n",
    "    for group in groups:\n",
    "        # metric_string.append(\"\\t{}\".format(group))\n",
    "        \n",
    "        for metric in metrics:\n",
    "            \n",
    "            metric_string.append(\"\\t {} {:<15} {} -> {}\".format(group, metric + \":\", \n",
    "                                                             sorted_df[key.format('Pre', group, metric)].iloc[idx],\n",
    "                                                             sorted_df[key.format('Post', group, metric)].iloc[idx]))\n",
    "\n",
    "    metric_strings.append(metric_string)\n",
    "assert len(metric_strings) == n_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get paths to all things we want to visualize: \n",
    "\n",
    "1) cumulative masking graphic\n",
    "2) cumulative masking graph\n",
    "3) class distribution pre/post edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get graphics from segmentation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy files from segmentation process\n",
    "file_names = [\n",
    "    '{}_cumulative_modifying.png'.format(data_type),\n",
    "    'target_{}_v_n_images.png'.format(data_type)]\n",
    "input_dirs = [os.path.dirname(path) for path in sorted_value_image_paths]\n",
    "html_asset_save_dirs, html_asset_save_paths, html_asset_save_ids = save_visualizations_separately(\n",
    "    input_dirs=input_dirs,\n",
    "    file_names=file_names,\n",
    "    output_dir=html_assets_dir,\n",
    "    overwrite=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bar Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create class distribution bar graphs per row\n",
    "columns = ['Pre Class Dist', 'Post Class Dist']\n",
    "bar_graph_save_paths = []\n",
    "for idx, (trial_dir, html_asset_save_dir) in enumerate(zip(sorted_trial_paths, html_asset_save_dirs)):\n",
    "    image_id = os.path.join(os.path.basename(os.path.dirname(html_asset_save_dir)),\n",
    "                            os.path.basename(html_asset_save_dir))\n",
    "    assert image_id in trial_dir\n",
    "    \n",
    "    data = []\n",
    "    for column in columns:\n",
    "        data.append(sorted_df.iloc[idx][column])\n",
    "    data = np.stack(data, axis=0)\n",
    "    \n",
    "    bar_graph_save_path = os.path.join(html_asset_save_dir, 'class_distribution_bar_graph.png')\n",
    "    bar_graph_save_paths.append(bar_graph_save_path)\n",
    "    \n",
    "    bar_graph(\n",
    "        data=data,\n",
    "        labels=class_list,\n",
    "        groups=columns,\n",
    "        title='Class Distribution for {}'.format(image_id),\n",
    "        xlabel_rotation=30,\n",
    "        ylabel='Counts',\n",
    "        save_path=bar_graph_save_path,\n",
    "        show_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_paths = []\n",
    "for vis_paths, bar_path in zip(html_asset_save_paths, bar_graph_save_paths):\n",
    "    cur_paths = []\n",
    "    if type(vis_paths) == str:\n",
    "        cur_paths.append(vis_paths)\n",
    "    elif type(vis_paths) == list or type(vis_paths) == tuple:\n",
    "        cur_paths += vis_paths\n",
    "        \n",
    "    if type(bar_path) == str:\n",
    "        cur_paths.append(bar_path)\n",
    "    elif type(bar_path) == list or type(bar_path) == tuple:\n",
    "        cur_paths += bar_path\n",
    "    \n",
    "    asset_paths.append(cur_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_html(file_paths,\n",
    "               asset_ids,\n",
    "               html_save_path,\n",
    "               texts=None,\n",
    "               id_regex='/+[a-z0-9_]*\\-[a-z0-9_]*\\-[a-z0-9_]*/.*/'):\n",
    "    '''\n",
    "    Given paths to assets to embed, build HTML page\n",
    "\n",
    "    Arg(s):\n",
    "        file_paths : list[str]\n",
    "            paths to each asset (sorted to group assets together)\n",
    "        html_save_path : str\n",
    "            where the html file will be saved to\n",
    "        id_regex : str\n",
    "            Regular expression to extract ID\n",
    "\n",
    "    Returns:\n",
    "        html_string : str\n",
    "            html as a string\n",
    "    '''\n",
    "    n_data = len(file_paths)\n",
    "    # Create Airium object\n",
    "    air = Airium()\n",
    "\n",
    "    air('<!DOCTYPE html>')\n",
    "    with air.html(lang=\"pl\"):\n",
    "        # Set HTML header\n",
    "        with air.head():\n",
    "            air.meta(charset=\"utf-8\")\n",
    "            air.title(_t=\"Cumulative Image Visualization\")\n",
    "\n",
    "        # Set HTML body\n",
    "        text_idx = 0\n",
    "        with air.body():\n",
    "            prev_id = \"\"\n",
    "            for group_idx, group_paths in enumerate(file_paths):\n",
    "                # asset_id = os.path.join(\n",
    "                #     os.path.basename(os.path.dirname(path)),\n",
    "                #     os.path.basename(path))\n",
    "                asset_id = asset_ids[group_idx]#re.search(id_regex, path).group()\n",
    "                # Remove the start and trailing backslashes\n",
    "                # Create new header\n",
    "                # if asset_id != prev_id:\n",
    "                with air.h3():\n",
    "                    air(\"{}/{}. {}\".format(group_idx+1, n_data, asset_id))\n",
    "                if texts is not None:\n",
    "                    for text in texts[group_idx]:\n",
    "                        with air.p():\n",
    "                            air(text)\n",
    "                # prev_id = asset_id\n",
    "                for asset_path in group_paths:\n",
    "                    # Embed asset as image\n",
    "                    relative_asset_path = os.path.relpath(asset_path, os.path.dirname(html_save_path))\n",
    "                    air.img(src=relative_asset_path, height=350)\n",
    "                    air.p(\"\\n\\n\")\n",
    "    # Turn Airium object to html string\n",
    "    html_string = str(air)\n",
    "    return html_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_file_name = \"sort_\"\n",
    "for sort_criteria in sort_columns:\n",
    "    html_file_name += sort_criteria.lower().replace(' ', '_')\n",
    "    html_file_name += '_'\n",
    "html_file_name += 'visualization.html'\n",
    "html_save_path = os.path.join(html_save_dir, html_file_name)\n",
    "html_string = build_html(\n",
    "    asset_paths,\n",
    "    asset_ids=sorted_IDs,\n",
    "    texts=metric_strings,\n",
    "    html_save_path=html_save_path)           \n",
    "\n",
    "with open(html_save_path, 'wb') as f:\n",
    "    f.write(bytes(html_string, encoding='utf-8'))\n",
    "print(\"Saved HTML file to {}\".format(html_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "editing",
   "language": "python",
   "name": "editing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
