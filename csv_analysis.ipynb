{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Template to Quickly Test Things Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "# import torch\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "sys.path.insert(0, 'src')\n",
    "from utils import read_json, read_lists\n",
    "# from utils.model_utils import prepare_device\n",
    "# # from parse_config import ConfigParser\n",
    "# from data_loader import data_loaders\n",
    "# import model.model as module_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants, paths\n",
    "config_path = 'configs/'\n",
    "timestamp = '0112_121958'\n",
    "csv_path = os.path.join('saved', 'edit', 'trials', 'CINIC10_ImageNet-VGG_16', timestamp, 'results_table.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV loaded from saved/edit/trials/CINIC10_ImageNet-VGG_16/0112_121958/results_table.csv\n",
      "37 rows\n"
     ]
    }
   ],
   "source": [
    "# Load CSV as pandas dataframe\n",
    "df = pd.read_csv(csv_path)\n",
    "n_total = len(df)\n",
    "print(\"CSV loaded from {}\".format(csv_path))\n",
    "print(\"{} rows\".format(n_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Round all numbers to 3 decimal places\n",
    "df.round(3)\n",
    "mean_df = df.mean()\n",
    "std_df = df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric                         Pre-Edit        Post-Edit           \n",
      "{} Accuracy                    0.687           0.677(0.018)\n",
      "{} Mean Precision              0.692           0.682(0.032)\n",
      "{} Mean Recall                 0.687           0.677(0.018)\n",
      "{} Mean F1                     0.684           0.672(0.025)\n",
      "\n",
      "{} Target Precision            0.702           0.590(0.107)\n",
      "{} Target Recall               0.430           0.495(0.049)\n",
      "{} Target F1                   0.533           0.528(0.036)\n",
      "\n",
      "{} Orig Pred Precision         0.655           0.671(0.264)\n",
      "{} Orig Pred Recall            0.656           0.484(0.214)\n",
      "{} Orig Pred F1                0.655           0.554(0.226)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = [['{} Accuracy', '{} Mean Precision', '{} Mean Recall', '{} Mean F1'], \n",
    "           ['{} Target Precision', '{} Target Recall', '{} Target F1'],\n",
    "           ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1']]\n",
    "print(\"{:<30} {:<15} {:<20}\".format(\"Metric\", \"Pre-Edit\", \"Post-Edit\"))\n",
    "for row in metrics:\n",
    "    for metric in row:\n",
    "        print(\"{:<30} {:<15.3f} {:.3f}({:.3f})\".format(\n",
    "            metric, mean_df[metric.format(\"Pre\")],\n",
    "            mean_df[metric.format(\"Post\")], std_df[metric.format(\"Post\")]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 1: Masked modifications will have greater changes than noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric                         Pre-Edit        Masked               Gaussian            \n",
      "{} Accuracy                    0.687           0.674(0.020)        0.685(0.006)\n",
      "{} Mean Precision              0.692           0.677(0.037)        0.694(0.002)\n",
      "{} Mean Recall                 0.687           0.674(0.020)        0.685(0.006)\n",
      "{} Mean F1                     0.684           0.668(0.029)        0.682(0.007)\n",
      "\n",
      "{} Target Precision            0.706           0.566(0.112)        0.669(0.045)\n",
      "{} Target Recall               0.427           0.506(0.047)        0.454(0.032)\n",
      "{} Target F1                   0.532           0.524(0.041)        0.539(0.009)\n",
      "\n",
      "{} Orig Pred Precision         0.654           0.653(0.300)        0.720(0.124)\n",
      "{} Orig Pred Recall            0.661           0.449(0.231)        0.580(0.126)\n",
      "{} Orig Pred F1                0.657           0.523(0.249)        0.637(0.120)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare mean post edit accuracy, precision, recall, and f1\n",
    "mean_masked = masked_rows.mean()\n",
    "mean_gaussian = gaussian_rows.mean()\n",
    "std_masked = masked_rows.std()\n",
    "std_gaussian = gaussian_rows.std()\n",
    "# print(mean_masked)\n",
    "\n",
    "metrics = [['{} Accuracy', '{} Mean Precision', '{} Mean Recall', '{} Mean F1'], \n",
    "           ['{} Target Precision', '{} Target Recall', '{} Target F1'],\n",
    "           ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1']]\n",
    "print(\"{:<30} {:<15} {:<20} {:<20}\".format(\"Metric\", \"Pre-Edit\", \"Masked\", \"Gaussian\"))\n",
    "for row in metrics:\n",
    "    for metric in row:\n",
    "        print(\"{:<30} {:<15.3f} {:.3f}({:.3f}) {:<6} {:.3f}({:.3f})\".format(\n",
    "            metric, mean_masked[metric.format(\"Pre\")],\n",
    "            mean_masked[metric.format(\"Post\")], std_masked[metric.format(\"Post\")], \"\",\n",
    "            mean_gaussian[metric.format(\"Post\")], std_gaussian[metric.format(\"Post\")]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 2: How many edits actually improved all three metrics for target class?\n",
    "\n",
    "Will these also incur larger harm in overall metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 edits improved all target metrics \n",
      "34 edits improved target recall\n",
      "26 edits improved target F1\n",
      "3 edits improved target precision\n",
      "1 edits improved target precision + f1\n",
      "25 edits improved target recall + f1\n",
      "0 edits improved target precision + recall\n"
     ]
    }
   ],
   "source": [
    "improve_target_rows = df[\n",
    "    (df['Post Target Precision'] > df['Pre Target Precision']) &\n",
    "    (df['Post Target Recall'] > df ['Pre Target Recall']) & \n",
    "    (df['Post Target F1'] > df['Pre Target F1'])]      \n",
    "\n",
    "improve_target_recall = df[df['Post Target Recall'] > df ['Pre Target Recall']] \n",
    "improve_target_f1 = df[df['Post Target F1'] > df ['Pre Target F1']] \n",
    "improve_target_precision = df[df['Post Target Precision'] > df ['Pre Target Precision']] \n",
    "    \n",
    "improve_target_precision_f1 = pd.merge(improve_target_precision, improve_target_f1, how='inner', on=['ID'])\n",
    "improve_target_recall_f1 = pd.merge(improve_target_recall, improve_target_f1, how='inner', on=['ID'])\n",
    "improve_target_recall_and_precision = pd.merge(improve_target_recall, improve_target_precision, how='inner', on=['ID'])\n",
    "\n",
    "print(\"{} edits improved all target metrics \".format(len(improve_target_rows)))\n",
    "print(\"{} edits improved target recall\".format(len(improve_target_recall)))\n",
    "print(\"{} edits improved target F1\".format(len(improve_target_f1)))\n",
    "print(\"{} edits improved target precision\".format(len(improve_target_precision)))\n",
    "print(\"{} edits improved target precision + f1\".format(len(improve_target_precision_f1)))\n",
    "print(\"{} edits improved target recall + f1\".format(len(improve_target_recall_f1)))\n",
    "print(\"{} edits improved target precision + recall\".format(len(improve_target_recall_and_precision)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 3: If the edit improved the target class (let's say F1), then the metrics of the originally predicted class will be worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mean metrics in original predicted class for rows that improved f1 in target class\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      "{} Orig Pred Precision         0.645(0.106)        0.695(0.176)\n",
      "{} Orig Pred Recall            0.644(0.107)        0.532(0.177)\n",
      "{} Orig Pred F1                0.644(0.104)        0.596(0.174)\n",
      "Average mean metrics in original predicted class for rows that did NOT improve f1 in target class\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      "{} Orig Pred Precision         0.678(0.112)        0.615(0.411)\n",
      "{} Orig Pred Recall            0.686(0.089)        0.372(0.258)\n",
      "{} Orig Pred F1                0.680(0.092)        0.456(0.305)\n"
     ]
    }
   ],
   "source": [
    "orig_metrics = ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1']\n",
    "mean_improve_target_f1 = improve_target_f1.mean()\n",
    "std_improve_target_f1 = improve_target_f1.std()\n",
    "\n",
    "print(\"Average mean metrics in original predicted class for rows that improved f1 in target class\")\n",
    "print(\"{:<30} {:<15} {:<20}\".format(\"Metric\", \"Pre-Edit\", \"Post-Edit\"))\n",
    "for metric in orig_metrics:\n",
    "    print(\"{:<30} {:.3f}({:.3f}) {:<6} {:.3f}({:.3f})\".format(\n",
    "        metric, \n",
    "        mean_improve_target_f1[metric.format(\"Pre\")], \n",
    "        std_improve_target_f1[metric.format(\"Pre\")], \n",
    "        \"\",\n",
    "        mean_improve_target_f1[metric.format(\"Post\")],\n",
    "        std_improve_target_f1[metric.format(\"Post\")]))\n",
    "    \n",
    "not_improve_target_f1 = df[~df.isin(improve_target_f1)].dropna()\n",
    "mean_not_improve_target_f1 = not_improve_target_f1.mean()\n",
    "std_not_improve_target_f1 = not_improve_target_f1.std()\n",
    "\n",
    "print(\"Average mean metrics in original predicted class for rows that did NOT improve f1 in target class\")\n",
    "print(\"{:<30} {:<15} {:<20}\".format(\"Metric\", \"Pre-Edit\", \"Post-Edit\"))\n",
    "for metric in orig_metrics:\n",
    "    print(\"{:<30} {:.3f}({:.3f}) {:<6} {:.3f}({:.3f})\".format(\n",
    "        metric, \n",
    "        mean_not_improve_target_f1[metric.format(\"Pre\")], \n",
    "        std_not_improve_target_f1[metric.format(\"Pre\")], \n",
    "        \"\",\n",
    "        mean_not_improve_target_f1[metric.format(\"Post\")],\n",
    "        std_not_improve_target_f1[metric.format(\"Post\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 4: If a gaussian noise segment is producing sucessful change, will the masked segment as well?\n",
    "\n",
    "Result: not necessarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 masked modifications\n",
      "10 Gaussian modifications\n",
      "3/10 gaussian modifications have corresponding masked segment as success:\n"
     ]
    }
   ],
   "source": [
    "# Count number of rows that are gaussian noise and are masked\n",
    "masked_rows = df[df['ID'].str.contains('masked')]\n",
    "n_masked = len(masked_rows)\n",
    "\n",
    "gaussian_rows = df[df['ID'].str.contains('gaussian')]\n",
    "n_gaussian = len(gaussian_rows)\n",
    "print(\"{} masked modifications\\n{} Gaussian modifications\".format(n_masked, n_gaussian))\n",
    "\n",
    "# For gaussian rows, are their corresponding masked segment also there?\n",
    "gaussian_IDs = gaussian_rows['ID']\n",
    "corresponding_masked_IDs = gaussian_IDs.replace('gaussian', 'masked', regex=True)\n",
    "# print(corresponding_masked_IDs)\n",
    "\n",
    "segments_with_both_gaussian_and_masked = list(set(corresponding_masked_IDs) & set(df['ID']))\n",
    "n_both = len(segments_with_both_gaussian_and_masked)\n",
    "print(\"{}/{} gaussian modifications have corresponding masked segment as success:\".format(n_both, n_gaussian))\n",
    "# print(segments_with_both_gaussian_and_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 5: Smaller segments will produce smaller changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 6: The neighbors of the value images should be less affected than the neighbors of the key images\n",
    "\n",
    "Compare distance between key-keyN and val-valN before and after the edit. The difference should be smaller for val-valN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features\n",
      "\tKey -> Key Neighbors: 0.782 ==> 0.755\n",
      "\t\tMean difference: -0.027 (-3.43%)\n",
      "\tVal -> Val Neighbors: 0.979 ==> 0.895\n",
      "\t\tMean difference: -0.084 (-8.57%)\n",
      "Logits\n",
      "\tKey -> Key Neighbors: 1.065 ==> 0.990\n",
      "\t\tMean difference: -0.075 (-7.04%)\n",
      "\tVal -> Val Neighbors: 1.342 ==> 1.134\n",
      "\t\tMean difference: -0.208 (-15.52%)\n"
     ]
    }
   ],
   "source": [
    "# mean_diff_key_keyN = (df['Post key-keyN (F)'] - df['Pre key-keyN (F)']).mean()\n",
    "mean_df = df.mean()\n",
    "for data_type in [\"F\", \"L\"]:\n",
    "    mean_pre_key_keyN = mean_df['Pre key-keyN ({})'.format(data_type)]\n",
    "    mean_post_key_keyN = mean_df['Post key-keyN ({})'.format(data_type)]\n",
    "    diff_key_keyN = mean_post_key_keyN - mean_pre_key_keyN\n",
    "    percent_diff_key_keyN = diff_key_keyN / mean_pre_key_keyN * 100\n",
    "    # mean_diff_val_valN = (df['Post val-valN (F)'] - df['Pre val-valN (F)']).mean()\n",
    "    mean_pre_val_valN = mean_df['Pre val-valN ({})'.format(data_type)]\n",
    "    mean_post_val_valN = mean_df['Post val-valN ({})'.format(data_type)]\n",
    "    diff_val_valN = mean_post_val_valN - mean_pre_val_valN\n",
    "    percent_diff_val_valN = diff_val_valN / mean_pre_val_valN * 100\n",
    "\n",
    "    print(\"{}\".format(\"Features\" if data_type == 'F' else \"Logits\"))\n",
    "    print(\"\\tKey -> Key Neighbors: {:.3f} ==> {:.3f}\".format(mean_pre_key_keyN, mean_post_key_keyN))\n",
    "    print(\"\\t\\tMean difference: {:.3f} ({:.2f}%)\".format(diff_key_keyN, percent_diff_key_keyN))\n",
    "    print(\"\\tVal -> Val Neighbors: {:.3f} ==> {:.3f}\".format(mean_pre_val_valN, mean_post_val_valN))\n",
    "    print(\"\\t\\tMean difference: {:.3f} ({:.2f}%)\".format(diff_val_valN, percent_diff_val_valN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 6: Edits that shared an original image will have smaller spread in post edit metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16    0.687371\n",
      "17    0.688529\n",
      "18    0.669086\n",
      "19    0.620000\n",
      "20    0.685500\n",
      "21    0.685386\n",
      "22    0.620614\n",
      "23    0.687300\n",
      "Name: Post Accuracy, dtype: float64\n",
      "14    0.641871\n",
      "Name: Post Accuracy, dtype: float64\n",
      "36    0.672886\n",
      "Name: Post Accuracy, dtype: float64\n",
      "15    0.675343\n",
      "Name: Post Accuracy, dtype: float64\n",
      "24    0.687429\n",
      "25    0.636157\n",
      "26    0.687714\n",
      "27    0.685071\n",
      "28    0.688229\n",
      "Name: Post Accuracy, dtype: float64\n",
      "6    0.668386\n",
      "Name: Post Accuracy, dtype: float64\n",
      "Metric                         STD Overall     Avg STD Grouped by Key Image\n",
      "Post Accuracy                  0.018           0.014\n",
      "Post Mean Precision            0.032           0.019\n",
      "Post Mean Recall               0.018           0.014\n",
      "Post Mean F1                   0.025           0.019\n",
      "\n",
      "Post Target Precision          0.107           0.088\n",
      "Post Target Recall             0.049           0.044\n",
      "Post Target F1                 0.036           0.025\n",
      "\n",
      "Post Orig Pred Precision       0.264           0.178\n",
      "Post Orig Pred Recall          0.214           0.153\n",
      "Post Orig Pred F1              0.226           0.143\n",
      "\n",
      "Post key-val (F)               0.559           0.413\n",
      "\n",
      "['dog-train-n02114712_211', 'dog-train-n02113624_8298', 'dog-train-n02089232_8735']\n"
     ]
    }
   ],
   "source": [
    "# Obtain unique key images\n",
    "ids = list(df['ID'])\n",
    "unique_keys = set()\n",
    "for image_id in ids:\n",
    "    key_id = image_id.split('/')[0]\n",
    "    unique_keys.add(key_id)\n",
    "    \n",
    "metrics = [['{} Accuracy', '{} Mean Precision', '{} Mean Recall', '{} Mean F1'], \n",
    "           ['{} Target Precision', '{} Target Recall', '{} Target F1'],\n",
    "           ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1'],\n",
    "           ['{} key-val (F)']]\n",
    "\n",
    "\n",
    "key_image_stds = pd.DataFrame()\n",
    "small_spread_keys = []\n",
    "for unique_key in unique_keys:\n",
    "    cur_rows = df[df['ID'].str.contains(unique_key)]\n",
    "    mean_cur_rows = cur_rows.mean()\n",
    "    std_cur_rows = cur_rows.std()\n",
    "    # print(\"Accuracy STD: {:.3f}\".format(std_cur_rows['Post Accuracy']))\n",
    "    # print(\"Precision STD: {:.3f}\".format(std_cur_rows['Post Mean Precision']))\n",
    "    # print(\"Recall STD: {:.3f}\".format(std_cur_rows['Post Mean Recall']))\n",
    "    if std_cur_rows['Post Accuracy'] < 0.01:\n",
    "        small_spread_keys.append(unique_key)\n",
    "    else:\n",
    "        print(cur_rows['Post Accuracy'])\n",
    "    if len(cur_rows) > 1:\n",
    "        key_image_stds = key_image_stds.append(std_cur_rows, ignore_index=True)\n",
    "        \n",
    "\n",
    "mean_key_image_stds = key_image_stds.mean()\n",
    "print(\"{:<30} {:<15} {:<20}\".format(\"Metric\", \"STD Overall\", \"Avg STD Grouped by Key Image\"))\n",
    "for row in metrics:\n",
    "    for metric in row:\n",
    "        print(\"{:<30} {:<15.3f} {:.3f}\".format(\n",
    "            metric.format(\"Post\"), std_df[metric.format(\"Post\")],\n",
    "            mean_key_image_stds[metric.format(\"Post\")]))\n",
    "    print(\"\")\n",
    "\n",
    "# print(\"Average accuracy standard deviation for an image: {}\".format(key_image_stds['Post Accuracy']))\n",
    "print(small_spread_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 9: Edits that share a original prediction will have more similar post edit metric results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2, 3, 4, 6, 7}\n",
      "0.004919442891969285\n",
      "0.006148371534599434\n",
      "0.024926112445238086\n",
      "0.006064829466900532\n",
      "0.028172920852970768\n"
     ]
    }
   ],
   "source": [
    "unique_original_predictions = set(df['Pre key Prediction'])\n",
    "print(unique_original_predictions)\n",
    "\n",
    "for og_prediction in unique_original_predictions:\n",
    "    cur_rows = df[df['Pre key Prediction'] == og_prediction]\n",
    "    # print(len(cur_rows))\n",
    "    mean_cur_rows = cur_rows.mean()\n",
    "    std_cur_rows = cur_rows.std()\n",
    "    print(std_cur_rows['Post Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "editing",
   "language": "python",
   "name": "editing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
