{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the combined CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "# import torch\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "sys.path.insert(0, 'src')\n",
    "from utils import read_json, read_lists, ensure_dir\n",
    "from utils.df_utils import load_and_preprocess_csv\n",
    "from utils.visualizations import histogram, bar_graph\n",
    "# from utils.model_utils import prepare_device\n",
    "# # from parse_config import ConfigParser\n",
    "# from data_loader import data_loaders\n",
    "# import model.model as module_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants, paths\n",
    "config_path = 'configs/'\n",
    "# timestamp = '0112_121958' # 37 dogs\n",
    "# timestamp = '0112_163516' # 128 cats\n",
    "# timestamp = '0113_160154' # 1800 dogs\n",
    "# timestamp = '0118_144235' # 1800 dogs w/ class dist\n",
    "# timestamp = '0120_155829' # 105 cats, semantic segmentation\n",
    "timestamp = '0125_114341'  # 50 cats, only felzenszwalb, semantic\n",
    "# csv_path = os.path.join('saved', 'edit', 'trials', 'CINIC10_ImageNet-VGG_16', timestamp, 'results_table.csv')\n",
    "# csv_path = os.path.join('saved', 'edit', 'trials', 'CINIC10_ImageNet-VGG_16', 'airplane_100', '0127_103716', 'results_table.csv')\n",
    "csv_path = 'saved/edit/trials/CINIC10_ImageNet-VGG_16/automobile_100/0127_103716/results_table.csv'\n",
    "print(csv_path)\n",
    "graph_save_dir = os.path.join(os.path.dirname(csv_path), 'graphs')\n",
    "ensure_dir(graph_save_dir)\n",
    "class_list_path = 'metadata/cinic-10/class_names.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read class list\n",
    "class_list = read_lists(class_list_path)\n",
    "# Load CSV as pandas dataframe\n",
    "# df = pd.read_csv(csv_path)\n",
    "df = load_and_preprocess_csv(\n",
    "    csv_path=csv_path,\n",
    "    drop_duplicates=['ID'],\n",
    "    round_to=3)\n",
    "    \n",
    "\n",
    "\n",
    "n_total = len(df)\n",
    "print(\"CSV loaded from {}\".format(csv_path))\n",
    "print(\"{} rows\".format(n_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Round all numbers to 3 decimal places\n",
    "mean_df = df.mean()\n",
    "std_df = df.std()\n",
    "\n",
    "# Need to remove rows with only 9 classes in predicted distribution for now\n",
    "post_class_dist = df['Post Class Dist']\n",
    "idxs = []\n",
    "for idx, row in enumerate(post_class_dist):\n",
    "    if row.shape[0] == 10:\n",
    "        idxs.append(idx)\n",
    "    else:\n",
    "        print(idx, row)\n",
    "# df = df.loc[df['Post Class Dist'].shape[0] > 0]\n",
    "# print(idxs)\n",
    "print(\"{} rows\".format(len(df)))\n",
    "df = df.iloc[idxs]\n",
    "print(\"{} rows after removing incomplete class distributions\".format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(df_, metrics=None):\n",
    "    if metrics == None:\n",
    "        metrics = [['{} Accuracy', '{} Mean Precision', '{} Mean Recall', '{} Mean F1'], \n",
    "               ['{} Target Precision', '{} Target Recall', '{} Target F1'],\n",
    "               ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1']]\n",
    "    assert type(metrics) == list and type(metrics[0]) == list, \"Metrics must be a 2-D list\"\n",
    "    \n",
    "    mean_df = df_.mean()\n",
    "    std_df = df_.std()\n",
    "    print(\"{:<30} {:<15} {:<20}\".format(\"Metric\", \"Pre-Edit\", \"Post-Edit\"))\n",
    "    for row in metrics:\n",
    "        for metric in row:\n",
    "            print(\"{:<30} {:<15.3f} {:.3f}({:.3f})\".format(\n",
    "                metric.format(\"\"), mean_df[metric.format(\"Pre\")],\n",
    "                mean_df[metric.format(\"Post\")], std_df[metric.format(\"Post\")]))\n",
    "        print(\"\")\n",
    "        \n",
    "def print_summaries(df_list, headers, metrics=None):\n",
    "    if metrics == None:\n",
    "        metrics = [['{} Accuracy', '{} Mean Precision', '{} Mean Recall', '{} Mean F1'], \n",
    "               ['{} Target Precision', '{} Target Recall', '{} Target F1'],\n",
    "               ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1']]\n",
    "    assert type(metrics) == list and type(metrics[0]) == list, \"Metrics must be a 2-D list\"\n",
    "    \n",
    "    assert len(df_list) == len(headers)\n",
    "    \n",
    "    # Calculate means and stds\n",
    "    means_list = [df_.mean() for df_ in df_list]\n",
    "    mean0 = means_list[0]\n",
    "    stds_list = [df_.std() for df_ in df_list]\n",
    "    \n",
    "    # Form and print header\n",
    "    header_string = \"{:<30} {:<15}\".format(\"Metric\", \"Pre-Edit\")\n",
    "    for header in headers:\n",
    "        header_string += \" {:<17}\".format(header)\n",
    "    print(header_string)\n",
    "    \n",
    "    for row in metrics:\n",
    "        for metric in row:\n",
    "            row_string = \"{:<30} {:<15.3f}\".format(\n",
    "                metric.format(\"\"), mean0[metric.format(\"Pre\")])\n",
    "            for mean, std in zip(means_list, stds_list):\n",
    "                row_string += \" {:.3f}({:.3f})  \\t\".format(\n",
    "                    mean[metric.format(\"Post\")],\n",
    "                    std[metric.format(\"Post\")])\n",
    "                \n",
    "            print(row_string)\n",
    "        print(\"\")\n",
    "    \n",
    "def get_unique_key_images(df_):\n",
    "    # Obtain unique key images\n",
    "    ids = list(df_['ID'])\n",
    "    unique_keys = set()\n",
    "    for image_id in ids:\n",
    "        key_id = image_id.split('/')[0]\n",
    "        unique_keys.add(key_id)\n",
    "    return unique_keys\n",
    "\n",
    "# def histogram(data,\n",
    "#               n_bins=10,\n",
    "#               data_range=None,\n",
    "#               color=None,\n",
    "#               title=None,\n",
    "#               xlabel=None,\n",
    "#               ylabel=None,\n",
    "#               marker=None,\n",
    "#               save_path=None):\n",
    "#     plt.hist(data,\n",
    "#              bins=n_bins,\n",
    "#              range=data_range,\n",
    "#              color=color)\n",
    "    \n",
    "#     # Marker is a vertical line marking original\n",
    "#     if marker is not None:\n",
    "#         plt.axvline(x=marker, color='r')\n",
    "        \n",
    "#     # Set title and axes labels\n",
    "#     if title is not None:\n",
    "#         plt.title(title)\n",
    "#     if xlabel is not None:\n",
    "#         plt.xlabel(xlabel)\n",
    "#     if ylabel is not None:\n",
    "#         plt.ylabel(ylabel)\n",
    "        \n",
    "#     if save_path is not None:\n",
    "#         ensure_dir(os.path.dirname(save_path))\n",
    "#         plt.savefig(save_path)\n",
    "#     plt.show()\n",
    "\n",
    "def summary_histogram(df_,\n",
    "                     metrics=None,\n",
    "                     n_bins=10,\n",
    "                     save_dir=None,\n",
    "                     tag=None):\n",
    "    '''\n",
    "    Display/save histograms of distributions of the metrics provided\n",
    "    '''\n",
    "    \n",
    "    if metrics is None:\n",
    "        metrics = [['{} Accuracy', '{} Mean Precision', '{} Mean Recall', '{} Mean F1'], \n",
    "                   ['{} Target Precision', '{} Target Recall', '{} Target F1'],\n",
    "                   ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1']]\n",
    "\n",
    "    mean_df_ = df_.mean()\n",
    "    for row in metrics:\n",
    "        for metric in row:\n",
    "            pre_metric_mean = mean_df_[metric.format(\"Pre\")]\n",
    "            post_metric = df_[metric.format(\"Post\")].to_numpy()\n",
    "            # Create save directory\n",
    "            if save_dir is not None:\n",
    "                if tag is None:\n",
    "                    save_path = os.path.join(save_dir, metric.format(\"\").strip())\n",
    "                else:\n",
    "                    save_path = os.path.join(save_dir, tag + \"_\" + metric.format(\"\").strip())\n",
    "            else:\n",
    "                save_path = None\n",
    "            \n",
    "            histogram(\n",
    "                data=post_metric,\n",
    "                n_bins=n_bins,\n",
    "                marker=pre_metric_mean,\n",
    "                title=metric.format(\"Post\"),\n",
    "                xlabel=metric.split(\" \", maxsplit=1)[1],\n",
    "                ylabel=\"Counts\",\n",
    "                save_path=save_path)\n",
    "            \n",
    "def orig_pred_histogram(df_,\n",
    "                        class_list,\n",
    "                        metrics=None,\n",
    "                        n_bins=10,\n",
    "                        save_dir=None):\n",
    "    \n",
    "    if metrics is None:\n",
    "        metrics = ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1']\n",
    "    \n",
    "    # Get unique class idxs\n",
    "    unique_orig_classes = set(df['Pre key Prediction'].to_numpy())\n",
    "\n",
    "    for class_idx in unique_orig_classes:\n",
    "        for metric in orig_pred_metrics:\n",
    "            # pre_metric = df[metric.format(\"Pre\")].to_numpy()\n",
    "            if save_dir is not None:\n",
    "                save_path = os.path.join(save_dir, class_list[class_idx], metric.format(\"\").strip())\n",
    "            else:\n",
    "                save_path = None\n",
    "            \n",
    "            # Only look at subset where original class prediction is current class\n",
    "            marker = df.loc[df[\"Pre key Prediction\"] == class_idx, metric.format(\"Pre\")].mean()\n",
    "            data = df.loc[df[\"Pre key Prediction\"] == class_idx, metric.format(\"Post\")]\n",
    "            histogram(\n",
    "                data=data,\n",
    "                n_bins=n_bins,\n",
    "                title=metric.format(\"Post\") + \" ({})\".format(class_list[class_idx]),\n",
    "                xlabel=metric.split(\" \", maxsplit=1)[1],\n",
    "                ylabel=\"Counts\",\n",
    "                marker=marker,\n",
    "                save_path=save_path\n",
    "            )\n",
    "\n",
    "def stratify_histogram(df_,\n",
    "                       column_name,\n",
    "                       class_list=None,\n",
    "                       metrics=None,\n",
    "                       n_bins=10,\n",
    "                       save_dir=None):\n",
    "    \n",
    "    assert column_name in df_\n",
    "    if metrics is None:\n",
    "        metrics = ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1']\n",
    "    \n",
    "    # Get unique class idxs\n",
    "    unique_values = set(df[column_name].to_numpy())\n",
    "    if len(unique_values) > 25:\n",
    "        print(\"Warning... {} unique values. That's a lot!\".format(len(unique_values)))\n",
    "\n",
    "    for unique_value in unique_values:\n",
    "        # Create unique ID for this value\n",
    "        try:\n",
    "            unique_id = class_list[unique_value]\n",
    "        except:\n",
    "            unique_id = unique_value\n",
    "        \n",
    "        # Iterate through metrics and make histogram for each\n",
    "        for metric in orig_pred_metrics:\n",
    "            if save_dir is not None:                \n",
    "                save_path = os.path.join(save_dir, unique_id, metric.format(\"\").strip())\n",
    "            else:\n",
    "                save_path = None\n",
    "            \n",
    "            # Only look at subset where original class prediction is current class\n",
    "            marker = df.loc[df[column_name] == unique_value, metric.format(\"Pre\")].mean()\n",
    "            data = df.loc[df[column_name] == unique_value, metric.format(\"Post\")]\n",
    "            histogram(\n",
    "                data=data,\n",
    "                n_bins=n_bins,\n",
    "                title=metric.format(\"Post\") + \" ({})\".format(unique_id),\n",
    "                xlabel=metric.split(\" \", maxsplit=1)[1],\n",
    "                ylabel=\"Counts\",\n",
    "                marker=marker,\n",
    "                save_path=save_path\n",
    "            )\n",
    "\n",
    "def class_distribution_bar_graph(df_, \n",
    "                                 class_list, \n",
    "                                 columns=None,\n",
    "                                 title=None,\n",
    "                                 save_path=None):\n",
    "    if columns is None:\n",
    "        columns = ['Pre Class Dist', 'Post Class Dist']\n",
    "    if title is None:\n",
    "        title = 'Class Distribution'\n",
    "    data = []\n",
    "    for column in columns:\n",
    "        column_data = np.array(list(df_[column]))\n",
    "        mean_column_data = np.mean(column_data, axis=0)\n",
    "        data.append(mean_column_data)\n",
    "    data = np.stack(data, axis=0)\n",
    "    \n",
    "    print(data)\n",
    "    \n",
    "    bar_graph(\n",
    "        data=data,\n",
    "        labels=class_list,\n",
    "        groups=columns,\n",
    "        title=title,\n",
    "        xlabel_rotation=30,\n",
    "        ylabel='Counts',\n",
    "        save_path=save_path)\n",
    "    \n",
    "    \n",
    "def mean_numpy_series(series, axis=0):\n",
    "    '''\n",
    "    Given a series of numpy arrays, return the mean\n",
    "\n",
    "    Arg(s):\n",
    "        series : pd.Series\n",
    "            series to take mean of\n",
    "        axis : int\n",
    "            axis across which to take the mean. Default is 0\n",
    "\n",
    "    '''\n",
    "    data = np.array(list(series))\n",
    "    return np.mean(data, axis=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [['{} Accuracy', '{} Mean Precision', '{} Mean Recall', '{} Mean F1'], \n",
    "           ['{} Target Precision', '{} Target Recall', '{} Target F1'],\n",
    "           ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1']]\n",
    "# Print overall summary\n",
    "print_summary(df)\n",
    "summary_save_dir = os.path.join(graph_save_dir, 'summary')\n",
    "ensure_dir(summary_save_dir)\n",
    "# Display bar graph of class distributions\n",
    "class_distribution_bar_graph(df,\n",
    "     class_list=class_list,\n",
    "     columns=['Pre Class Dist', 'Post Class Dist'],\n",
    "     title=\"Average Class Distribution\",\n",
    "     save_path=os.path.join(summary_save_dir, 'class_dist.png'))\n",
    "# Display histogram of overall\n",
    "\n",
    "\n",
    "summary_histogram(df,\n",
    "              metrics=metrics,\n",
    "              n_bins=50,\n",
    "              save_dir=summary_save_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histograms per original class prediction\n",
    "orig_pred_metrics = ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1']\n",
    "orig_pred_save_dir = os.path.join(graph_save_dir, 'original_predictions')\n",
    "stratify_histogram(df,\n",
    "                   column_name=\"Pre key Prediction\",\n",
    "                   class_list=class_list,\n",
    "                   metrics=orig_pred_metrics,\n",
    "                   n_bins=25,\n",
    "                   save_dir=orig_pred_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 1: Masked modifications will have greater changes than noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_rows = df[df['ID'].str.contains('masked')]\n",
    "n_masked = len(masked_rows)\n",
    "\n",
    "gaussian_rows = df[df['ID'].str.contains('gaussian')]\n",
    "n_gaussian = len(gaussian_rows)\n",
    "# Compare mean post edit accuracy, precision, recall, and f1\n",
    "mean_masked = masked_rows.mean()\n",
    "mean_gaussian = gaussian_rows.mean()\n",
    "std_masked = masked_rows.std()\n",
    "std_gaussian = gaussian_rows.std()\n",
    "# print(mean_masked)\n",
    "\n",
    "metrics = [['{} Accuracy', '{} Mean Precision', '{} Mean Recall', '{} Mean F1'], \n",
    "           ['{} Target Precision', '{} Target Recall', '{} Target F1'],\n",
    "           ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1']]\n",
    "print(\"{:<30} {:<15} {:<20} {:<20}\".format(\"Metric\", \"Pre-Edit\", \"Masked\", \"Gaussian\"))\n",
    "for row in metrics:\n",
    "    for metric in row:\n",
    "        print(\"{:<30} {:<15.3f} {:.3f}({:.3f}) {:<6} {:.3f}({:.3f})\".format(\n",
    "            metric.format(\"\"), mean_masked[metric.format(\"Pre\")],\n",
    "            mean_masked[metric.format(\"Post\")], std_masked[metric.format(\"Post\")], \"\",\n",
    "            mean_gaussian[metric.format(\"Post\")], std_gaussian[metric.format(\"Post\")]))\n",
    "    print(\"\")\n",
    "    \n",
    "# Bar graph comparing original / post masked / post gaussian\n",
    "data = []\n",
    "groups = ['Pre Edit', 'Masked Edit', 'Gaussian Edit']\n",
    "data.append(mean_numpy_series(df['Pre Class Dist']))\n",
    "data.append(mean_numpy_series(masked_rows['Post Class Dist']))\n",
    "data.append(mean_numpy_series(gaussian_rows['Post Class Dist']))\n",
    "\n",
    "data = np.stack(data, axis=0)\n",
    "bar_graph(\n",
    "        data=data,\n",
    "        labels=class_list,\n",
    "        groups=groups,\n",
    "        title='Masked v Gaussian',\n",
    "        xlabel_rotation=30,\n",
    "        ylabel='Counts',\n",
    "        save_path=os.path.join(graph_save_dir, 'class_dist_masked_gaussian.png'))\n",
    "# class_distribution_bar_graph(masked_rows, \n",
    "#                              class_list=class_list, \n",
    "#                              title=\"Masked Class Distributions\")\n",
    "\n",
    "# class_distribution_bar_graph(gaussian_rows, \n",
    "#                              class_list=class_list, \n",
    "#                              title=\"Gaussian Class Distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 2: How many edits actually improved all three metrics for target class?\n",
    "\n",
    "Will these also incur larger harm in overall metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improve_target_rows = df[\n",
    "    (df['Post Target Precision'] > df['Pre Target Precision']) &\n",
    "    (df['Post Target Recall'] > df ['Pre Target Recall']) & \n",
    "    (df['Post Target F1'] > df['Pre Target F1'])]      \n",
    "\n",
    "improve_target_recall = df[df['Post Target Recall'] > df ['Pre Target Recall']] \n",
    "improve_target_f1 = df[df['Post Target F1'] > df ['Pre Target F1']] \n",
    "improve_target_precision = df[df['Post Target Precision'] > df ['Pre Target Precision']] \n",
    "    \n",
    "improve_target_precision_f1 = pd.merge(improve_target_precision, improve_target_f1, how='inner', on=['ID'])\n",
    "improve_target_recall_f1 = pd.merge(improve_target_recall, improve_target_f1, how='inner', on=['ID'])\n",
    "improve_target_recall_and_precision = pd.merge(improve_target_recall, improve_target_precision, how='inner', on=['ID'])\n",
    "\n",
    "print(\"{} edits improved all target metrics (P/R/F1) \".format(len(improve_target_rows)))\n",
    "print(\"{} edits improved target recall\".format(len(improve_target_recall)))\n",
    "print(\"{} edits improved target F1\".format(len(improve_target_f1)))\n",
    "print(\"{} edits improved target precision\".format(len(improve_target_precision)))\n",
    "print(\"{} edits improved target precision + f1\".format(len(improve_target_precision_f1)))\n",
    "print(\"{} edits improved target recall + f1\".format(len(improve_target_recall_f1)))\n",
    "print(\"{} edits improved target precision + recall\".format(len(improve_target_recall_and_precision)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 3: If the edit improved the target class (let's say F1), then the metrics of the originally predicted class will be worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(graph_save_dir, 'hyp3_f1')\n",
    "orig_metrics = ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1']\n",
    "improve_target_f1 = df[df['Post Target F1'] > df ['Pre Target F1']] \n",
    "mean_improve_target_f1 = improve_target_f1.mean()\n",
    "std_improve_target_f1 = improve_target_f1.std()\n",
    "\n",
    "print(\"Average mean metrics in original predicted class for rows that improved f1 in target class ({} samples)\".format(len(improve_target_f1)))\n",
    "print(\"{:<30} {:<15} {:<20}\".format(\"Metric\", \"Pre-Edit\", \"Post-Edit\"))\n",
    "for metric in orig_metrics:\n",
    "    print(\"{:<30} {:.3f}({:.3f}) {:<6} {:.3f}({:.3f})\".format(\n",
    "        metric.format(\"\"), \n",
    "        mean_improve_target_f1[metric.format(\"Pre\")], \n",
    "        std_improve_target_f1[metric.format(\"Pre\")], \n",
    "        \"\",\n",
    "        mean_improve_target_f1[metric.format(\"Post\")],\n",
    "        std_improve_target_f1[metric.format(\"Post\")]))\n",
    "    \n",
    "    \n",
    "# summary_histogram(\n",
    "#     improve_target_f1,\n",
    "#     metrics=[orig_metrics],\n",
    "#     save_dir=save_dir,\n",
    "#     tag='improve_target_f1')\n",
    "\n",
    "# did not improve target f1 rows\n",
    "not_improve_target_f1 = df[~df.isin(improve_target_f1)].dropna()\n",
    "mean_not_improve_target_f1 = not_improve_target_f1.mean()\n",
    "std_not_improve_target_f1 = not_improve_target_f1.std()\n",
    "\n",
    "print(\"Average mean metrics in original predicted class for rows that did NOT improve f1 in target class({} samples)\".format(len(not_improve_target_f1)))\n",
    "print(\"{:<30} {:<15} {:<20}\".format(\"Metric\", \"Pre-Edit\", \"Post-Edit\"))\n",
    "for metric in orig_metrics:\n",
    "    print(\"{:<30} {:.3f}({:.3f}) {:<6} {:.3f}({:.3f})\".format(\n",
    "        metric.format(\"\"), \n",
    "        mean_not_improve_target_f1[metric.format(\"Pre\")], \n",
    "        std_not_improve_target_f1[metric.format(\"Pre\")], \n",
    "        \"\",\n",
    "        mean_not_improve_target_f1[metric.format(\"Post\")],\n",
    "        std_not_improve_target_f1[metric.format(\"Post\")]))\n",
    "# summary_histogram(\n",
    "#     not_improve_target_f1,\n",
    "#     metrics=[orig_metrics],\n",
    "#     save_dir=save_dir,\n",
    "#     tag='not_improve_target_f1')\n",
    "\n",
    "# Graph histogram of improved target f1 and did not improve target f1 on original predicted class metrics together\n",
    "labels = ['Improved Target F1', 'Did NOT Improve Target F1']\n",
    "for metric in orig_metrics:\n",
    "    save_path = os.path.join(save_dir, metric.format(\"\").strip())\n",
    "    data = []\n",
    "    data.append(improve_target_f1[metric.format(\"Post\")])\n",
    "    data.append(not_improve_target_f1[metric.format(\"Post\")])   \n",
    "    marker = mean_improve_target_f1[metric.format(\"Pre\")]\n",
    "    histogram(\n",
    "        data=data,\n",
    "        marker=marker,\n",
    "        xlabel=metric.format(\"\"),\n",
    "        ylabel='Counts',\n",
    "        title='{} between improved target f1 and not improved target f1'.format(metric.format(\"\")),\n",
    "        labels=labels,\n",
    "        save_path=save_path)\n",
    "\n",
    "# Bar graph of post edit class distributions\n",
    "data = []\n",
    "groups = ['Pre Edit', 'Improved Target F1', 'Did Not Improved Target F1']\n",
    "data.append(mean_numpy_series(df['Pre Class Dist']))\n",
    "data.append(mean_numpy_series(improve_target_f1['Post Class Dist']))\n",
    "data.append(mean_numpy_series(not_improve_target_f1['Post Class Dist']))\n",
    "\n",
    "data = np.stack(data, axis=0)\n",
    "bar_graph(\n",
    "        data=data,\n",
    "        labels=class_list,\n",
    "        groups=groups,\n",
    "        title='Class Distribution of Improve Target F1 and Not',\n",
    "        xlabel_rotation=30,\n",
    "        ylabel='Counts',\n",
    "        save_path=os.path.join(save_dir, 'class_dist.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = 'Recall'\n",
    "pos_target_pos_orig = df[ \n",
    "    (df['Post Target {}'.format(metric_name)] > df['Pre Target {}'.format(metric_name)]) &\n",
    "    (df['Post Orig Pred {}'.format(metric_name)] > df['Pre Orig Pred {}'.format(metric_name)])]  \n",
    "\n",
    "pos_target_neg_orig = df[ \n",
    "    (df['Post Target {}'.format(metric_name)] > df['Pre Target {}'.format(metric_name)]) &\n",
    "    (df['Post Orig Pred {}'.format(metric_name)] < df['Pre Orig Pred {}'.format(metric_name)])] \n",
    "\n",
    "neg_target_pos_orig = df[ \n",
    "    (df['Post Target {}'.format(metric_name)] < df['Pre Target {}'.format(metric_name)]) &\n",
    "    (df['Post Orig Pred {}'.format(metric_name)] > df['Pre Orig Pred {}'.format(metric_name)])]  \n",
    "\n",
    "neg_target_neg_orig = df[ \n",
    "    (df['Post Target {}'.format(metric_name)] < df['Pre Target {}'.format(metric_name)]) &\n",
    "    (df['Post Orig Pred {}'.format(metric_name)] < df['Pre Orig Pred {}'.format(metric_name)])]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metric: {}\".format(metric_name))\n",
    "print(\"Pos Target Pos Orig: {}\".format(len(pos_target_pos_orig)))\n",
    "print(\"Pos Target Neg Orig: {}\".format(len(pos_target_neg_orig)))\n",
    "print(\"Neg Target Pos Orig: {}\".format(len(neg_target_pos_orig)))\n",
    "print(\"Neg Target Neg Orig: {}\".format(len(neg_target_neg_orig)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 4: If a gaussian noise segment is producing sucessful change, will the masked segment as well? In those cases, do the masked ones or noised ones perform better?\n",
    "\n",
    "Result: not necessarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of rows that are gaussian noise and are masked\n",
    "masked_rows = df[df['ID'].str.contains('masked')]\n",
    "n_masked = len(masked_rows)\n",
    "\n",
    "gaussian_rows = df[df['ID'].str.contains('gaussian')]\n",
    "n_gaussian = len(gaussian_rows)\n",
    "print(\"{} masked modifications\\n{} Gaussian modifications\".format(n_masked, n_gaussian))\n",
    "\n",
    "# For gaussian rows, are their corresponding masked segment also there?\n",
    "gaussian_IDs = gaussian_rows['ID']\n",
    "corresponding_masked_IDs = gaussian_IDs.replace('gaussian', 'masked', regex=True)\n",
    "# print(corresponding_masked_IDs)\n",
    "\n",
    "masked_IDs_with_both_gaussian_and_masked = list(set(corresponding_masked_IDs) & set(df['ID']))\n",
    "gaussian_IDs_with_both_gaussian_and_masked = [s.replace('masked', 'gaussian') for s in masked_IDs_with_both_gaussian_and_masked]\n",
    "n_both = len(masked_IDs_with_both_gaussian_and_masked)\n",
    "print(\"{}/{} gaussian modifications have corresponding masked segment as success:\".format(n_both, n_gaussian))\n",
    "\n",
    "print(\"\\nOf data with both masked and gaussian modifications producing changed prediction:\")\n",
    "\n",
    "masked_rows = df.loc[df['ID'].isin(masked_IDs_with_both_gaussian_and_masked)]\n",
    "print(\"Masked results ({} rows):\".format(len(masked_rows)))\n",
    "print_summary(masked_rows)\n",
    "\n",
    "gaussian_rows = df.loc[df['ID'].isin(gaussian_IDs_with_both_gaussian_and_masked)]\n",
    "print(\"Gaussian results ({} rows):\".format(len(gaussian_rows)))\n",
    "\n",
    "print_summary(gaussian_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 5: Smaller segments will produce smaller changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 6: The neighbors of the value images should be less affected than the neighbors of the key images\n",
    "\n",
    "Compare distance between key-keyN and val-valN before and after the edit. The difference should be smaller for val-valN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_diff_key_keyN = (df['Post key-keyN (F)'] - df['Pre key-keyN (F)']).mean()\n",
    "mean_df = df.mean()\n",
    "for data_type in [\"F\", \"L\"]:\n",
    "    mean_pre_key_keyN = mean_df['Pre key-keyN ({})'.format(data_type)]\n",
    "    mean_post_key_keyN = mean_df['Post key-keyN ({})'.format(data_type)]\n",
    "    diff_key_keyN = mean_post_key_keyN - mean_pre_key_keyN\n",
    "    percent_diff_key_keyN = diff_key_keyN / mean_pre_key_keyN * 100\n",
    "    k_n_increase = len(df[df['Pre key-keyN ({})'.format(data_type)] < df['Post key-keyN ({})'.format(data_type)]])\n",
    "    k_n_decrease = len(df) - k_n_increase\n",
    "    # mean_diff_val_valN = (df['Post val-valN (F)'] - df['Pre val-valN (F)']).mean()\n",
    "    mean_pre_val_valN = mean_df['Pre val-valN ({})'.format(data_type)]\n",
    "    mean_post_val_valN = mean_df['Post val-valN ({})'.format(data_type)]\n",
    "    diff_val_valN = mean_post_val_valN - mean_pre_val_valN\n",
    "    percent_diff_val_valN = diff_val_valN / mean_pre_val_valN * 100\n",
    "    v_n_increase = len(df[df['Pre val-valN ({})'.format(data_type)] < df['Post val-valN ({})'.format(data_type)]])\n",
    "    v_n_decrease = len(df) - v_n_increase\n",
    "\n",
    "    print(\"{}\".format(\"Features\" if data_type == 'F' else \"Logits\"))\n",
    "    print(\"\\tKey -> Key Neighbors: {:.3f} ==> {:.3f}\".format(mean_pre_key_keyN, mean_post_key_keyN))\n",
    "    print(\"\\t\\tMean difference: {:.3f} ({:.2f}%)\".format(diff_key_keyN, percent_diff_key_keyN))\n",
    "    print(\"\\t\\tNum increase: {} Num decrease: {}\".format(k_n_increase, k_n_decrease))\n",
    "    print(\"\\tVal -> Val Neighbors: {:.3f} ==> {:.3f}\".format(mean_pre_val_valN, mean_post_val_valN))\n",
    "    print(\"\\t\\tMean difference: {:.3f} ({:.2f}%)\".format(diff_val_valN, percent_diff_val_valN))\n",
    "    print(\"\\t\\tNum increase: {} Num decrease: {}\".format(v_n_increase, v_n_decrease))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 7: Edits that shared an original image will have smaller spread in post edit metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain unique key images\n",
    "\n",
    "unique_keys = get_unique_key_images(df)\n",
    "print(\"{} unique key images\".format(len(unique_keys)))\n",
    "\n",
    "metrics = [['{} Accuracy', '{} Mean Precision', '{} Mean Recall', '{} Mean F1'], \n",
    "           ['{} Target Precision', '{} Target Recall', '{} Target F1'],\n",
    "           ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1'],\n",
    "           # ['{} key-val (F)'],\n",
    "          ]\n",
    "\n",
    "\n",
    "key_image_stds = pd.DataFrame()\n",
    "small_spread_keys = []\n",
    "for unique_key in unique_keys:\n",
    "    cur_rows = df[df['ID'].str.contains(unique_key)]\n",
    "    mean_cur_rows = cur_rows.mean()\n",
    "    std_cur_rows = cur_rows.std()\n",
    "    # print(\"Accuracy STD: {:.3f}\".format(std_cur_rows['Post Accuracy']))\n",
    "    # print(\"Precision STD: {:.3f}\".format(std_cur_rows['Post Mean Precision']))\n",
    "    # print(\"Recall STD: {:.3f}\".format(std_cur_rows['Post Mean Recall']))\n",
    "    if std_cur_rows['Post Accuracy'] < 0.04:\n",
    "        small_spread_keys.append(unique_key)\n",
    "    # else:\n",
    "    #     print(cur_rows['Post Accuracy'])\n",
    "    if len(cur_rows) > 1:\n",
    "        key_image_stds = key_image_stds.append(std_cur_rows, ignore_index=True)\n",
    "        \n",
    "\n",
    "mean_key_image_stds = key_image_stds.mean()\n",
    "print(\"{:<30} {:<15} {:<20}\".format(\"Metric\", \"STD Overall\", \"Avg STD Grouped by Key Image\"))\n",
    "for row in metrics:\n",
    "    for metric in row:\n",
    "        print(\"{:<30} {:<15.3f} {:.3f} ({:.2f}%)\".format(\n",
    "            metric.format(\"Post\"), std_df[metric.format(\"Post\")],\n",
    "            mean_key_image_stds[metric.format(\"Post\")],\n",
    "            100 * mean_key_image_stds[metric.format(\"Post\")] / std_df[metric.format(\"Post\")]))\n",
    "    print(\"\")\n",
    "\n",
    "# print(\"Average accuracy standard deviation for an image: {}\".format(key_image_stds['Post Accuracy']))\n",
    "# print(small_spread_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 8: When grouping by same key image, is there a trend to variation in post edit distances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keys = get_unique_key_images(df)\n",
    "\n",
    "metrics = [['{} key-val (F)'], \n",
    "           ['{} key-valN (F)', '{} val-keyN (F)', '{} key-keyN (F)', '{} val-valN (F)']\n",
    "           ]\n",
    "print(\"Overall distance summaries\")\n",
    "print_summary(df, metrics=metrics)\n",
    "keys_small_dist_stds = []\n",
    "small_std_rows = pd.DataFrame()\n",
    "random_std_rows = pd.DataFrame()\n",
    "for unique_key in unique_keys:\n",
    "    # print(\"Distance summaries for {}\".format(unique_key))\n",
    "    cur_rows = df[df['ID'].str.contains(unique_key)]\n",
    "    mean_cur_rows = cur_rows.mean()\n",
    "    std_cur_rows = cur_rows.std()\n",
    "    # print_summary(cur_rows, metrics=metrics)\n",
    "    # Store this key if all its stds are < overall\n",
    "    do_store = True\n",
    "    for metric_row in metrics:\n",
    "        for metric in metric_row:\n",
    "            metric = metric.format(\"Post\")\n",
    "            if std_df[metric] < std_cur_rows[metric]:\n",
    "                do_store = False\n",
    "        \n",
    "    if do_store:\n",
    "        keys_small_dist_stds.append(unique_key)\n",
    "        small_std_rows = small_std_rows.append(std_cur_rows, ignore_index=True)\n",
    "        \n",
    "    # Random subsets\n",
    "    random_idxs = np.random.randint(len(df), size=len(cur_rows))\n",
    "    random_rows = df.iloc[random_idxs]\n",
    "    random_std_rows = random_std_rows.append(random_rows.std(), ignore_index=True)\n",
    "    \n",
    "        \n",
    "mean_small_std_rows = small_std_rows.mean()\n",
    "# mean_random_std_rows = random_std_rows.mean()\n",
    "print(\"{:<30} {:<15} {:<20} {:<20}\".format(\"Metric\", \"STD Overall\", \"Avg STD by Key\", \"Random subsets\"))\n",
    "for row in metrics:\n",
    "    for metric in row:\n",
    "        print(\"{:<30} {:<15.3f} {:.3f} ({:.2f}%)\\t    {:<15.3f}\".format(\n",
    "            metric.format(\"Post\"), std_df[metric.format(\"Post\")],\n",
    "            mean_small_std_rows[metric.format(\"Post\")],\n",
    "            100 * mean_small_std_rows[metric.format(\"Post\")] / std_df[metric.format(\"Post\")],\n",
    "            mean_random_std_rows[metric.format(\"Post\")]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 9: Edits that share a original prediction will have more similar post edit metric results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [['{} Accuracy', '{} Mean Precision', '{} Mean Recall', '{} Mean F1'], \n",
    "           ['{} Target Precision', '{} Target Recall', '{} Target F1'],\n",
    "           ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1'],\n",
    "           # ['{} key-val (F)'],\n",
    "          ]\n",
    "\n",
    "unique_original_predictions = set(df['Pre key Prediction'])\n",
    "\n",
    "unique_originals_stds = pd.DataFrame()\n",
    "for og_prediction in unique_original_predictions:\n",
    "    cur_rows = df[df['Pre key Prediction'] == og_prediction]\n",
    "    if len(cur_rows) > 1:\n",
    "        unique_originals_stds = unique_originals_stds.append(std_cur_rows, ignore_index=True)\n",
    "    # print(len(cur_rows))\n",
    "    mean_cur_rows = cur_rows.mean()\n",
    "    std_cur_rows = cur_rows.std()\n",
    "#     print(\"Original prediction: {}\".format(og_prediction))\n",
    "#     print(\"N samples: {}\".format(len(cur_rows)))\n",
    "#     print_summary(cur_rows)\n",
    "    \n",
    "mean_unique_originals_stds = unique_originals_stds.mean()\n",
    "print(\"{:<30} {:<15} {:<20}\".format(\"Metric\", \"STD Overall\", \"Avg STD by Orig. Pred.\"))\n",
    "for row in metrics:\n",
    "    for metric in row:\n",
    "        print(\"{:<30} {:<15.3f} {:.3f} ({:.2f}%)\".format(\n",
    "            metric.format(\"Post\"), std_df[metric.format(\"Post\")],\n",
    "            mean_unique_originals_stds[metric.format(\"Post\")],\n",
    "            100 * mean_unique_originals_stds[metric.format(\"Post\")] / std_df[metric.format(\"Post\")]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 10: did any edits cause post edit distances to increase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    ['{} key-val (F)', '{} val-keyN (F)', '{} key-valN (F)', '{} key-keyN (F)', '{} val-valN (F)']\n",
    "]\n",
    "# Obtain for non exclusionary (may include rows where other distances increase too\n",
    "for metric in metrics[0]:\n",
    "    print(\"Metric:{}\".format(metric.format(\"\")))\n",
    "    kv_f = df[df[metric.format('Post')] > df[metric.format('Pre')]]\n",
    "\n",
    "        \n",
    "    print(\"{} edits caused distance to increase (among others)\".format(len(kv_f)))\n",
    "    print_summary(kv_f)\n",
    "    \n",
    "    # Ensure all other metrics decrease after edit\n",
    "    exc_kv_f = kv_f.copy()\n",
    "    for other in metrics[0]:\n",
    "        if other == metric:\n",
    "            continue\n",
    "        exc_kv_f = exc_kv_f[exc_kv_f[other.format(\"Post\")] < exc_kv_f[other.format(\"Pre\")]]\n",
    "    print(\"{} edits caused solely {} distance to increase \".format(len(exc_kv_f), metric.format(\"\")))\n",
    "    print_summary(exc_kv_f)\n",
    "    # print_summaries(\n",
    "    #     df_list=[kv_f, exc_kv_f],\n",
    "    #     headers=[\"Inclusive ({})\".format(len(kv_f)), \"Exclusive ({})\".format(len(exc_kv_f))])\n",
    "                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 11: How do the edits that improve recall of both target and original class differ from those that only improve target class (and decrease original class recall)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = 'F1'\n",
    "pos_target_pos_orig = df[ \n",
    "    (df['Post Target {}'.format(metric_name)] > df['Pre Target {}'.format(metric_name)]) &\n",
    "    (df['Post Orig Pred {}'.format(metric_name)] > df['Pre Orig Pred {}'.format(metric_name)])]  \n",
    "\n",
    "pos_target_neg_orig = df[ \n",
    "    (df['Post Target {}'.format(metric_name)] > df['Pre Target {}'.format(metric_name)]) &\n",
    "    (df['Post Orig Pred {}'.format(metric_name)] < df['Pre Orig Pred {}'.format(metric_name)])] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Increase target and orig. prediction\")\n",
    "print_summary(pos_target_pos_orig)\n",
    "\n",
    "    \n",
    "# get_unique(pos_target_pos_orig)\n",
    "\n",
    "print(\"Increase target and decrease orig. prediction\")\n",
    "print_summary(pos_target_neg_orig)\n",
    "# get_unique(pos_target_neg_orig)\n",
    "\n",
    "# Print how many neighbors became target\n",
    "keys = [\n",
    "    \"Num of key's Neighbors Became Target (F)\", \n",
    "    # \"Num of key's Neighbors Unaffected (F)\", \n",
    "    \"Num of key's Neighbors Became Target (L)\",\n",
    "    # \"Num of key's Neighbors Unaffected (L)\", \n",
    "    \"Num of val's Neighbors Became Target (F)\", \n",
    "    # \"Num of val's Neighbors Unaffected (F)\", \n",
    "    \"Num of val's Neighbors Became Target (L)\",\n",
    "    # \"Num of val's Neighbors Unaffected (L)\", \n",
    "]\n",
    "key_abbr = [\n",
    "    \"Key features\", \"Key logits\", \"Val features\", \"Val logits\"\n",
    "]\n",
    "print(\"Pos Target Pos Orig ({}):\".format(len(pos_target_pos_orig)))\n",
    "for key, abbr in zip(keys, key_abbr):\n",
    "    pp_key = pos_target_pos_orig[key]\n",
    "    # print(\"Key: {}\".format(key))\n",
    "    print(\"{} \\tmean: {:.3f} median: {}\".format(abbr, pp_key.mean(), pp_key.median()))\n",
    "    # print(pp_key)\n",
    "    \n",
    "print(\"Pos Target Neg Orig ({}):\".format(len(pos_target_neg_orig)))\n",
    "for key, abbr in zip(keys, key_abbr):\n",
    "    pn_key = pos_target_neg_orig[key]\n",
    "    print(\"{} \\tmean: {:.3f} median: {}\".format(abbr, pn_key.mean(), pn_key.median()))\n",
    "    # print(pn_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 12: Given the changes in neighbor predictions, will we be able to see patterns in performance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirds = [(0, 33), (34,66), (67, 100)]\n",
    "quarters = [(0, 25), (26, 50), (51, 75), (76, 100)]\n",
    "for (low, high) in thirds:\n",
    "    cur_third = df[((df[\"Num of key's Neighbors Became Target (F)\"] >= low) &\n",
    "                    (df[\"Num of key's Neighbors Became Target (F)\"] <= high))]\n",
    "\n",
    "    print(\"{} edits with {}-{} of key's neighbors becoming target\".format(len(cur_third), low, high))\n",
    "    print_summary(cur_third)\n",
    "    \n",
    "for (low, high) in quarters:\n",
    "    cur_third = df[((df[\"Num of val's Neighbors Became Target (F)\"] >= low) &\n",
    "                    (df[\"Num of val's Neighbors Became Target (F)\"] <= high))]\n",
    "\n",
    "    print(\"{} edits with {}-{} of val's neighbors becoming target\".format(len(cur_third), low, high))\n",
    "    print_summary(cur_third)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = df[\"Num of key's Neighbors Became Target (F)\"].to_numpy()\n",
    "metrics = [['{} Accuracy', '{} Mean Precision', '{} Mean Recall', '{} Mean F1'], \n",
    "           ['{} Target Precision', '{} Target Recall', '{} Target F1'],\n",
    "           ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1'],\n",
    "           # ['{} key-val (F)'],\n",
    "          ]\n",
    "# metric = \"Post Mean Recall\"\n",
    "# metric = \"Post Orig Pred Recall\"\n",
    "\n",
    "for row_idx, row in enumerate(metrics):\n",
    "    fig, axs = plt.subplots(len(row), figsize=(8, 16))\n",
    "    for idx, metric in enumerate(row):\n",
    "        ax = axs[idx]\n",
    "        metric = metric.format(\"Post\")\n",
    "        y_axis = df[metric].to_numpy()\n",
    "        ax.scatter(x_axis, y_axis)\n",
    "        ax.set_ylabel(metric)\n",
    "\n",
    "    plt.savefig('temp/{}.png'.format(row_idx))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "editing",
   "language": "python",
   "name": "editing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
