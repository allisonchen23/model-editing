{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Template to Quickly Test Things Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "# import torch\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "sys.path.insert(0, 'src')\n",
    "from utils import read_json, read_lists\n",
    "# from utils.model_utils import prepare_device\n",
    "# # from parse_config import ConfigParser\n",
    "# from data_loader import data_loaders\n",
    "# import model.model as module_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants, paths\n",
    "config_path = 'configs/'\n",
    "# timestamp = '0112_121958' # 37 dogs\n",
    "timestamp = '0112_163516' # 128 cats\n",
    "csv_path = os.path.join('saved', 'edit', 'trials', 'CINIC10_ImageNet-VGG_16', timestamp, 'results_table.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV loaded from saved/edit/trials/CINIC10_ImageNet-VGG_16/0112_163516/results_table.csv\n",
      "128 rows\n"
     ]
    }
   ],
   "source": [
    "# Load CSV as pandas dataframe\n",
    "df = pd.read_csv(csv_path)\n",
    "n_total = len(df)\n",
    "print(\"CSV loaded from {}\".format(csv_path))\n",
    "print(\"{} rows\".format(n_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Round all numbers to 3 decimal places\n",
    "df.round(3)\n",
    "mean_df = df.mean()\n",
    "std_df = df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(df_, metrics=None):\n",
    "    if metrics == None:\n",
    "        metrics = [['{} Accuracy', '{} Mean Precision', '{} Mean Recall', '{} Mean F1'], \n",
    "               ['{} Target Precision', '{} Target Recall', '{} Target F1'],\n",
    "               ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1']]\n",
    "    assert type(metrics) == list and type(metrics[0]) == list, \"Metrics must be a 2-D list\"\n",
    "    \n",
    "    mean_df = df_.mean()\n",
    "    std_df = df_.std()\n",
    "    print(\"{:<30} {:<15} {:<20}\".format(\"Metric\", \"Pre-Edit\", \"Post-Edit\"))\n",
    "    for row in metrics:\n",
    "        for metric in row:\n",
    "            print(\"{:<30} {:<15.3f} {:.3f}({:.3f})\".format(\n",
    "                metric.format(\"\"), mean_df[metric.format(\"Pre\")],\n",
    "                mean_df[metric.format(\"Post\")], std_df[metric.format(\"Post\")]))\n",
    "        print(\"\")\n",
    "        \n",
    "def get_unique_key_images(df_):\n",
    "    # Obtain unique key images\n",
    "    ids = list(df_['ID'])\n",
    "    unique_keys = set()\n",
    "    for image_id in ids:\n",
    "        key_id = image_id.split('/')[0]\n",
    "        unique_keys.add(key_id)\n",
    "    return unique_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Accuracy                      0.687           0.642(0.046)\n",
      " Mean Precision                0.692           0.665(0.050)\n",
      " Mean Recall                   0.687           0.642(0.046)\n",
      " Mean F1                       0.684           0.631(0.055)\n",
      "\n",
      " Target Precision              0.554           0.387(0.086)\n",
      " Target Recall                 0.549           0.693(0.080)\n",
      " Target F1                     0.551           0.485(0.058)\n",
      "\n",
      " Orig Pred Precision           0.681           0.474(0.409)\n",
      " Orig Pred Recall              0.633           0.258(0.266)\n",
      " Orig Pred F1                  0.647           0.309(0.304)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = [['{} Accuracy', '{} Mean Precision', '{} Mean Recall', '{} Mean F1'], \n",
    "           ['{} Target Precision', '{} Target Recall', '{} Target F1'],\n",
    "           ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1']]\n",
    "print(\"{:<30} {:<15} {:<20}\".format(\"Metric\", \"Pre-Edit\", \"Post-Edit\"))\n",
    "for row in metrics:\n",
    "    for metric in row:\n",
    "        print(\"{:<30} {:<15.3f} {:.3f}({:.3f})\".format(\n",
    "            metric.format(\"\"), mean_df[metric.format(\"Pre\")],\n",
    "            mean_df[metric.format(\"Post\")], std_df[metric.format(\"Post\")]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 1: Masked modifications will have greater changes than noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric                         Pre-Edit        Masked               Gaussian            \n",
      " Accuracy                      0.687           0.635(0.049)        0.663(0.028)\n",
      " Mean Precision                0.692           0.660(0.052)        0.681(0.038)\n",
      " Mean Recall                   0.687           0.635(0.049)        0.663(0.028)\n",
      " Mean F1                       0.684           0.623(0.058)        0.654(0.039)\n",
      "\n",
      " Target Precision              0.554           0.372(0.085)        0.436(0.070)\n",
      " Target Recall                 0.549           0.713(0.079)        0.629(0.039)\n",
      " Target F1                     0.551           0.477(0.060)        0.509(0.047)\n",
      "\n",
      " Orig Pred Precision           0.679           0.421(0.414)        0.649(0.343)\n",
      " Orig Pred Recall              0.638           0.219(0.255)        0.385(0.263)\n",
      " Orig Pred F1                  0.648           0.265(0.295)        0.453(0.291)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "masked_rows = df[df['ID'].str.contains('masked')]\n",
    "n_masked = len(masked_rows)\n",
    "\n",
    "gaussian_rows = df[df['ID'].str.contains('gaussian')]\n",
    "n_gaussian = len(gaussian_rows)\n",
    "# Compare mean post edit accuracy, precision, recall, and f1\n",
    "mean_masked = masked_rows.mean()\n",
    "mean_gaussian = gaussian_rows.mean()\n",
    "std_masked = masked_rows.std()\n",
    "std_gaussian = gaussian_rows.std()\n",
    "# print(mean_masked)\n",
    "\n",
    "metrics = [['{} Accuracy', '{} Mean Precision', '{} Mean Recall', '{} Mean F1'], \n",
    "           ['{} Target Precision', '{} Target Recall', '{} Target F1'],\n",
    "           ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1']]\n",
    "print(\"{:<30} {:<15} {:<20} {:<20}\".format(\"Metric\", \"Pre-Edit\", \"Masked\", \"Gaussian\"))\n",
    "for row in metrics:\n",
    "    for metric in row:\n",
    "        print(\"{:<30} {:<15.3f} {:.3f}({:.3f}) {:<6} {:.3f}({:.3f})\".format(\n",
    "            metric.format(\"\"), mean_masked[metric.format(\"Pre\")],\n",
    "            mean_masked[metric.format(\"Post\")], std_masked[metric.format(\"Post\")], \"\",\n",
    "            mean_gaussian[metric.format(\"Post\")], std_gaussian[metric.format(\"Post\")]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 2: How many edits actually improved all three metrics for target class?\n",
    "\n",
    "Will these also incur larger harm in overall metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 edits improved all target metrics (P/R/F1)\n",
      "127 edits improved target recall\n",
      "6 edits improved target F1\n",
      "0 edits improved target precision\n",
      "0 edits improved target precision + f1\n",
      "6 edits improved target recall + f1\n",
      "0 edits improved target precision + recall\n"
     ]
    }
   ],
   "source": [
    "improve_target_rows = df[\n",
    "    (df['Post Target Precision'] > df['Pre Target Precision']) &\n",
    "    (df['Post Target Recall'] > df ['Pre Target Recall']) & \n",
    "    (df['Post Target F1'] > df['Pre Target F1'])]      \n",
    "\n",
    "improve_target_recall = df[df['Post Target Recall'] > df ['Pre Target Recall']] \n",
    "improve_target_f1 = df[df['Post Target F1'] > df ['Pre Target F1']] \n",
    "improve_target_precision = df[df['Post Target Precision'] > df ['Pre Target Precision']] \n",
    "    \n",
    "improve_target_precision_f1 = pd.merge(improve_target_precision, improve_target_f1, how='inner', on=['ID'])\n",
    "improve_target_recall_f1 = pd.merge(improve_target_recall, improve_target_f1, how='inner', on=['ID'])\n",
    "improve_target_recall_and_precision = pd.merge(improve_target_recall, improve_target_precision, how='inner', on=['ID'])\n",
    "\n",
    "print(\"{} edits improved all target metrics (P/R/F1)\".format(len(improve_target_rows)))\n",
    "print(\"{} edits improved target recall\".format(len(improve_target_recall)))\n",
    "print(\"{} edits improved target F1\".format(len(improve_target_f1)))\n",
    "print(\"{} edits improved target precision\".format(len(improve_target_precision)))\n",
    "print(\"{} edits improved target precision + f1\".format(len(improve_target_precision_f1)))\n",
    "print(\"{} edits improved target recall + f1\".format(len(improve_target_recall_f1)))\n",
    "print(\"{} edits improved target precision + recall\".format(len(improve_target_recall_and_precision)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 3: If the edit improved the target class (let's say F1), then the metrics of the originally predicted class will be worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mean metrics in original predicted class for rows that improved f1 in target class (6 samples)\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Orig Pred Precision           0.705(0.064)        0.770(0.074)\n",
      " Orig Pred Recall              0.509(0.132)        0.449(0.164)\n",
      " Orig Pred F1                  0.584(0.095)        0.550(0.122)\n",
      "Average mean metrics in original predicted class for rows that did NOT improve f1 in target class(122 samples)\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Orig Pred Precision           0.680(0.074)        0.460(0.413)\n",
      " Orig Pred Recall              0.639(0.137)        0.248(0.267)\n",
      " Orig Pred F1                  0.650(0.091)        0.298(0.306)\n"
     ]
    }
   ],
   "source": [
    " orig_metrics = ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1']\n",
    "mean_improve_target_f1 = improve_target_f1.mean()\n",
    "std_improve_target_f1 = improve_target_f1.std()\n",
    "\n",
    "print(\"Average mean metrics in original predicted class for rows that improved f1 in target class ({} samples)\".format(len(improve_target_f1)))\n",
    "print(\"{:<30} {:<15} {:<20}\".format(\"Metric\", \"Pre-Edit\", \"Post-Edit\"))\n",
    "for metric in orig_metrics:\n",
    "    print(\"{:<30} {:.3f}({:.3f}) {:<6} {:.3f}({:.3f})\".format(\n",
    "        metric.format(\"\"), \n",
    "        mean_improve_target_f1[metric.format(\"Pre\")], \n",
    "        std_improve_target_f1[metric.format(\"Pre\")], \n",
    "        \"\",\n",
    "        mean_improve_target_f1[metric.format(\"Post\")],\n",
    "        std_improve_target_f1[metric.format(\"Post\")]))\n",
    "    \n",
    "not_improve_target_f1 = df[~df.isin(improve_target_f1)].dropna()\n",
    "mean_not_improve_target_f1 = not_improve_target_f1.mean()\n",
    "std_not_improve_target_f1 = not_improve_target_f1.std()\n",
    "\n",
    "print(\"Average mean metrics in original predicted class for rows that did NOT improve f1 in target class({} samples)\".format(len(not_improve_target_f1)))\n",
    "print(\"{:<30} {:<15} {:<20}\".format(\"Metric\", \"Pre-Edit\", \"Post-Edit\"))\n",
    "for metric in orig_metrics:\n",
    "    print(\"{:<30} {:.3f}({:.3f}) {:<6} {:.3f}({:.3f})\".format(\n",
    "        metric.format(\"\"), \n",
    "        mean_not_improve_target_f1[metric.format(\"Pre\")], \n",
    "        std_not_improve_target_f1[metric.format(\"Pre\")], \n",
    "        \"\",\n",
    "        mean_not_improve_target_f1[metric.format(\"Post\")],\n",
    "        std_not_improve_target_f1[metric.format(\"Post\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = 'Recall'\n",
    "pos_target_pos_orig = df[ \n",
    "    (df['Post Target {}'.format(metric_name)] > df['Pre Target {}'.format(metric_name)]) &\n",
    "    (df['Post Orig Pred {}'.format(metric_name)] > df['Pre Orig Pred {}'.format(metric_name)])]  \n",
    "\n",
    "pos_target_neg_orig = df[ \n",
    "    (df['Post Target {}'.format(metric_name)] > df['Pre Target {}'.format(metric_name)]) &\n",
    "    (df['Post Orig Pred {}'.format(metric_name)] < df['Pre Orig Pred {}'.format(metric_name)])] \n",
    "\n",
    "neg_target_pos_orig = df[ \n",
    "    (df['Post Target {}'.format(metric_name)] < df['Pre Target {}'.format(metric_name)]) &\n",
    "    (df['Post Orig Pred {}'.format(metric_name)] > df['Pre Orig Pred {}'.format(metric_name)])]  \n",
    "\n",
    "neg_target_neg_orig = df[ \n",
    "    (df['Post Target {}'.format(metric_name)] < df['Pre Target {}'.format(metric_name)]) &\n",
    "    (df['Post Orig Pred {}'.format(metric_name)] < df['Pre Orig Pred {}'.format(metric_name)])]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: Recall\n",
      "Pos Target Pos Orig: 7\n",
      "Pos Target Neg Orig: 120\n",
      "Neg Target Pos Orig: 0\n",
      "Neg Target Neg Orig: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Metric: {}\".format(metric_name))\n",
    "print(\"Pos Target Pos Orig: {}\".format(len(pos_target_pos_orig)))\n",
    "print(\"Pos Target Neg Orig: {}\".format(len(pos_target_neg_orig)))\n",
    "print(\"Neg Target Pos Orig: {}\".format(len(neg_target_pos_orig)))\n",
    "print(\"Neg Target Neg Orig: {}\".format(len(neg_target_neg_orig)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 4: If a gaussian noise segment is producing sucessful change, will the masked segment as well? In those cases, do the masked ones or noised ones perform better?\n",
    "\n",
    "Result: not necessarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 masked modifications\n",
      "30 Gaussian modifications\n",
      "14/30 gaussian modifications have corresponding masked segment as success:\n",
      "\n",
      "Of data with both masked and gaussian modifications producing changed prediction:\n",
      "Masked results (14 rows):\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Accuracy                      0.687           0.641(0.033)\n",
      " Mean Precision                0.692           0.670(0.046)\n",
      " Mean Recall                   0.687           0.641(0.033)\n",
      " Mean F1                       0.684           0.630(0.044)\n",
      "\n",
      " Target Precision              0.554           0.360(0.060)\n",
      " Target Recall                 0.549           0.734(0.052)\n",
      " Target F1                     0.551           0.478(0.043)\n",
      "\n",
      " Orig Pred Precision           0.661           0.402(0.423)\n",
      " Orig Pred Recall              0.567           0.220(0.265)\n",
      " Orig Pred F1                  0.601           0.268(0.305)\n",
      "\n",
      "Gaussian results (14 rows):\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Accuracy                      0.687           0.672(0.017)\n",
      " Mean Precision                0.692           0.680(0.038)\n",
      " Mean Recall                   0.687           0.672(0.017)\n",
      " Mean F1                       0.684           0.664(0.027)\n",
      "\n",
      " Target Precision              0.554           0.451(0.051)\n",
      " Target Recall                 0.549           0.633(0.034)\n",
      " Target F1                     0.551           0.523(0.028)\n",
      "\n",
      " Orig Pred Precision           0.661           0.598(0.331)\n",
      " Orig Pred Recall              0.567           0.392(0.241)\n",
      " Orig Pred F1                  0.601           0.464(0.265)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count number of rows that are gaussian noise and are masked\n",
    "masked_rows = df[df['ID'].str.contains('masked')]\n",
    "n_masked = len(masked_rows)\n",
    "\n",
    "gaussian_rows = df[df['ID'].str.contains('gaussian')]\n",
    "n_gaussian = len(gaussian_rows)\n",
    "print(\"{} masked modifications\\n{} Gaussian modifications\".format(n_masked, n_gaussian))\n",
    "\n",
    "# For gaussian rows, are their corresponding masked segment also there?\n",
    "gaussian_IDs = gaussian_rows['ID']\n",
    "corresponding_masked_IDs = gaussian_IDs.replace('gaussian', 'masked', regex=True)\n",
    "# print(corresponding_masked_IDs)\n",
    "\n",
    "masked_IDs_with_both_gaussian_and_masked = list(set(corresponding_masked_IDs) & set(df['ID']))\n",
    "gaussian_IDs_with_both_gaussian_and_masked = [s.replace('masked', 'gaussian') for s in masked_IDs_with_both_gaussian_and_masked]\n",
    "n_both = len(masked_IDs_with_both_gaussian_and_masked)\n",
    "print(\"{}/{} gaussian modifications have corresponding masked segment as success:\".format(n_both, n_gaussian))\n",
    "\n",
    "print(\"\\nOf data with both masked and gaussian modifications producing changed prediction:\")\n",
    "\n",
    "masked_rows = df.loc[df['ID'].isin(masked_IDs_with_both_gaussian_and_masked)]\n",
    "print(\"Masked results ({} rows):\".format(len(masked_rows)))\n",
    "print_summary(masked_rows)\n",
    "\n",
    "gaussian_rows = df.loc[df['ID'].isin(gaussian_IDs_with_both_gaussian_and_masked)]\n",
    "print(\"Gaussian results ({} rows):\".format(len(gaussian_rows)))\n",
    "\n",
    "print_summary(gaussian_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 5: Smaller segments will produce smaller changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 6: The neighbors of the value images should be less affected than the neighbors of the key images\n",
    "\n",
    "Compare distance between key-keyN and val-valN before and after the edit. The difference should be smaller for val-valN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features\n",
      "\tKey -> Key Neighbors: 0.819 ==> 0.727\n",
      "\t\tMean difference: -0.092 (-11.28%)\n",
      "\tVal -> Val Neighbors: 0.965 ==> 0.880\n",
      "\t\tMean difference: -0.085 (-8.83%)\n",
      "Logits\n",
      "\tKey -> Key Neighbors: 1.106 ==> 0.903\n",
      "\t\tMean difference: -0.203 (-18.38%)\n",
      "\tVal -> Val Neighbors: 1.308 ==> 1.086\n",
      "\t\tMean difference: -0.223 (-17.02%)\n"
     ]
    }
   ],
   "source": [
    "# mean_diff_key_keyN = (df['Post key-keyN (F)'] - df['Pre key-keyN (F)']).mean()\n",
    "mean_df = df.mean()\n",
    "for data_type in [\"F\", \"L\"]:\n",
    "    mean_pre_key_keyN = mean_df['Pre key-keyN ({})'.format(data_type)]\n",
    "    mean_post_key_keyN = mean_df['Post key-keyN ({})'.format(data_type)]\n",
    "    diff_key_keyN = mean_post_key_keyN - mean_pre_key_keyN\n",
    "    percent_diff_key_keyN = diff_key_keyN / mean_pre_key_keyN * 100\n",
    "    # mean_diff_val_valN = (df['Post val-valN (F)'] - df['Pre val-valN (F)']).mean()\n",
    "    mean_pre_val_valN = mean_df['Pre val-valN ({})'.format(data_type)]\n",
    "    mean_post_val_valN = mean_df['Post val-valN ({})'.format(data_type)]\n",
    "    diff_val_valN = mean_post_val_valN - mean_pre_val_valN\n",
    "    percent_diff_val_valN = diff_val_valN / mean_pre_val_valN * 100\n",
    "\n",
    "    print(\"{}\".format(\"Features\" if data_type == 'F' else \"Logits\"))\n",
    "    print(\"\\tKey -> Key Neighbors: {:.3f} ==> {:.3f}\".format(mean_pre_key_keyN, mean_post_key_keyN))\n",
    "    print(\"\\t\\tMean difference: {:.3f} ({:.2f}%)\".format(diff_key_keyN, percent_diff_key_keyN))\n",
    "    print(\"\\tVal -> Val Neighbors: {:.3f} ==> {:.3f}\".format(mean_pre_val_valN, mean_post_val_valN))\n",
    "    print(\"\\t\\tMean difference: {:.3f} ({:.2f}%)\".format(diff_val_valN, percent_diff_val_valN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 7: Edits that shared an original image will have smaller spread in post edit metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric                         STD Overall     Avg STD Grouped by Key Image\n",
      "Post Accuracy                  0.046           0.034\n",
      "Post Mean Precision            0.050           0.041\n",
      "Post Mean Recall               0.046           0.034\n",
      "Post Mean F1                   0.055           0.042\n",
      "\n",
      "Post Target Precision          0.086           0.069\n",
      "Post Target Recall             0.080           0.057\n",
      "Post Target F1                 0.058           0.046\n",
      "\n",
      "Post Orig Pred Precision       0.409           0.397\n",
      "Post Orig Pred Recall          0.266           0.213\n",
      "Post Orig Pred F1              0.304           0.253\n",
      "\n",
      "['cat-train-n02123597_10163', 'cat-train-n02129463_2015', 'cat-train-n02123478_8080', 'cat-train-n01322898_6579', 'cat-train-n02128757_4479', 'cat-train-n02129991_418', 'cat-train-n02129530_5462', 'cat-train-n02125311_6892', 'cat-train-n02124313_1518', 'cat-train-n02120997_15889', 'cat-train-n02128757_3571', 'cat-train-n02129604_11283', 'cat-train-n02123597_3714', 'cat-train-n02129165_3350', 'cat-train-n02129165_3224']\n"
     ]
    }
   ],
   "source": [
    "# Obtain unique key images\n",
    "# ids = list(df['ID'])\n",
    "# unique_keys = set()\n",
    "# for image_id in ids:\n",
    "#     key_id = image_id.split('/')[0]\n",
    "#     unique_keys.add(key_id)\n",
    "unique_keys = get_unique_key_images(df)\n",
    "metrics = [['{} Accuracy', '{} Mean Precision', '{} Mean Recall', '{} Mean F1'], \n",
    "           ['{} Target Precision', '{} Target Recall', '{} Target F1'],\n",
    "           ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1'],\n",
    "           # ['{} key-val (F)'],\n",
    "          ]\n",
    "\n",
    "\n",
    "key_image_stds = pd.DataFrame()\n",
    "small_spread_keys = []\n",
    "for unique_key in unique_keys:\n",
    "    cur_rows = df[df['ID'].str.contains(unique_key)]\n",
    "    mean_cur_rows = cur_rows.mean()\n",
    "    std_cur_rows = cur_rows.std()\n",
    "    # print(\"Accuracy STD: {:.3f}\".format(std_cur_rows['Post Accuracy']))\n",
    "    # print(\"Precision STD: {:.3f}\".format(std_cur_rows['Post Mean Precision']))\n",
    "    # print(\"Recall STD: {:.3f}\".format(std_cur_rows['Post Mean Recall']))\n",
    "    if std_cur_rows['Post Accuracy'] < 0.04:\n",
    "        small_spread_keys.append(unique_key)\n",
    "    # else:\n",
    "    #     print(cur_rows['Post Accuracy'])\n",
    "    if len(cur_rows) > 1:\n",
    "        key_image_stds = key_image_stds.append(std_cur_rows, ignore_index=True)\n",
    "        \n",
    "\n",
    "mean_key_image_stds = key_image_stds.mean()\n",
    "print(\"{:<30} {:<15} {:<20}\".format(\"Metric\", \"STD Overall\", \"Avg STD Grouped by Key Image\"))\n",
    "for row in metrics:\n",
    "    for metric in row:\n",
    "        print(\"{:<30} {:<15.3f} {:.3f}\".format(\n",
    "            metric.format(\"Post\"), std_df[metric.format(\"Post\")],\n",
    "            mean_key_image_stds[metric.format(\"Post\")]))\n",
    "    print(\"\")\n",
    "\n",
    "# print(\"Average accuracy standard deviation for an image: {}\".format(key_image_stds['Post Accuracy']))\n",
    "print(small_spread_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 8: When grouping by same key image, is there a trend to variation in post edit distances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall distance summaries\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " key-val (F)                   3.649           2.044(0.579)\n",
      "\n",
      " key-valN (F)                  3.820           2.252(0.451)\n",
      " val-keyN (F)                  3.794           2.216(0.481)\n",
      " key-keyN (F)                  0.819           0.727(0.334)\n",
      " val-valN (F)                  0.965           0.880(0.237)\n",
      "\n",
      "Metric                         STD Overall     Avg STD by Key Image Random subsets      \n",
      "Post key-val (F)               0.579           0.284                0.507          \n",
      "\n",
      "Post key-valN (F)              0.451           0.233                0.402          \n",
      "Post val-keyN (F)              0.481           0.249                0.426          \n",
      "Post key-keyN (F)              0.334           0.038                0.284          \n",
      "Post val-valN (F)              0.237           0.137                0.181          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_keys = get_unique_key_images(df)\n",
    "\n",
    "metrics = [['{} key-val (F)'], \n",
    "           ['{} key-valN (F)', '{} val-keyN (F)', '{} key-keyN (F)', '{} val-valN (F)']\n",
    "           ]\n",
    "print(\"Overall distance summaries\")\n",
    "print_summary(df, metrics=metrics)\n",
    "keys_small_dist_stds = []\n",
    "small_std_rows = pd.DataFrame()\n",
    "random_std_rows = pd.DataFrame()\n",
    "for unique_key in unique_keys:\n",
    "    # print(\"Distance summaries for {}\".format(unique_key))\n",
    "    cur_rows = df[df['ID'].str.contains(unique_key)]\n",
    "    mean_cur_rows = cur_rows.mean()\n",
    "    std_cur_rows = cur_rows.std()\n",
    "    # print_summary(cur_rows, metrics=metrics)\n",
    "    # Store this key if all its stds are < overall\n",
    "    do_store = True\n",
    "    for metric_row in metrics:\n",
    "        for metric in metric_row:\n",
    "            metric = metric.format(\"Post\")\n",
    "            if std_df[metric] < std_cur_rows[metric]:\n",
    "                do_store = False\n",
    "        \n",
    "    if do_store:\n",
    "        keys_small_dist_stds.append(unique_key)\n",
    "        small_std_rows = small_std_rows.append(std_cur_rows, ignore_index=True)\n",
    "        \n",
    "    # Random subsets\n",
    "    random_idxs = np.random.randint(len(df), size=len(cur_rows))\n",
    "    random_rows = df.iloc[random_idxs]\n",
    "    random_std_rows = random_std_rows.append(random_rows.std(), ignore_index=True)\n",
    "    \n",
    "        \n",
    "mean_small_std_rows = small_std_rows.mean()\n",
    "# mean_random_std_rows = random_std_rows.mean()\n",
    "print(\"{:<30} {:<15} {:<20} {:<20}\".format(\"Metric\", \"STD Overall\", \"Avg STD by Key Image\", \"Random subsets\"))\n",
    "for row in metrics:\n",
    "    for metric in row:\n",
    "        print(\"{:<30} {:<15.3f} {:<20.3f} {:<15.3f}\".format(\n",
    "            metric.format(\"Post\"), std_df[metric.format(\"Post\")],\n",
    "            mean_small_std_rows[metric.format(\"Post\")],\n",
    "            mean_random_std_rows[metric.format(\"Post\")]))\n",
    "    print(\"\")\n",
    "# print(\"{}/{} keys with distance STDs smaller than overall\".format(len(keys_small_dist_stds), len(unique_keys)))\n",
    "# print_summary(small_std_rows)\n",
    "# print_summary(small_std_rows, metrics=metrics)\n",
    "# large_std_rows = df[~df.isin(small_std_rows)].dropna()\n",
    "\n",
    "# print(\"{}/{} keys with distance STDs larger than overall\".format(len(get_unique_key_images(large_std_rows)), len(unique_keys)))\n",
    "# print_summary(large_std_rows)\n",
    "# print_summary(large_std_rows, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 9: Edits that share a original prediction will have more similar post edit metric results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2, 4, 5, 6, 7, 9}\n",
      "Metric                         STD Overall     Avg STD Grouped by Original Prediction\n",
      "Post Accuracy                  0.046           0.045\n",
      "Post Mean Precision            0.050           0.043\n",
      "Post Mean Recall               0.046           0.045\n",
      "Post Mean F1                   0.055           0.053\n",
      "\n",
      "Post Target Precision          0.086           0.083\n",
      "Post Target Recall             0.080           0.073\n",
      "Post Target F1                 0.058           0.057\n",
      "\n",
      "Post Orig Pred Precision       0.409           0.348\n",
      "Post Orig Pred Recall          0.266           0.226\n",
      "Post Orig Pred F1              0.304           0.263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = [['{} Accuracy', '{} Mean Precision', '{} Mean Recall', '{} Mean F1'], \n",
    "           ['{} Target Precision', '{} Target Recall', '{} Target F1'],\n",
    "           ['{} Orig Pred Precision', '{} Orig Pred Recall', '{} Orig Pred F1'],\n",
    "           # ['{} key-val (F)'],\n",
    "          ]\n",
    "\n",
    "unique_original_predictions = set(df['Pre key Prediction'])\n",
    "print(unique_original_predictions)\n",
    "\n",
    "unique_originals_stds = pd.DataFrame()\n",
    "for og_prediction in unique_original_predictions:\n",
    "    cur_rows = df[df['Pre key Prediction'] == og_prediction]\n",
    "    if len(cur_rows) > 1:\n",
    "        unique_originals_stds = unique_originals_stds.append(std_cur_rows, ignore_index=True)\n",
    "    # print(len(cur_rows))\n",
    "    mean_cur_rows = cur_rows.mean()\n",
    "    std_cur_rows = cur_rows.std()\n",
    "#     print(\"Original prediction: {}\".format(og_prediction))\n",
    "#     print(\"N samples: {}\".format(len(cur_rows)))\n",
    "#     print_summary(cur_rows)\n",
    "    \n",
    "mean_unique_originals_stds = unique_originals_stds.mean()\n",
    "print(\"{:<30} {:<15} {:<20}\".format(\"Metric\", \"STD Overall\", \"Avg STD Grouped by Original Prediction\"))\n",
    "for row in metrics:\n",
    "    for metric in row:\n",
    "        print(\"{:<30} {:<15.3f} {:.3f}\".format(\n",
    "            metric.format(\"Post\"), std_df[metric.format(\"Post\")],\n",
    "            mean_unique_originals_stds[metric.format(\"Post\")]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 10: did any edits cause post edit distances to increase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: key-val (F)\n",
      "['cat-train-n02128925_4745/quickshift_masked_3']\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Accuracy                      0.687           0.654(nan)\n",
      " Mean Precision                0.692           0.731(nan)\n",
      " Mean Recall                   0.687           0.655(nan)\n",
      " Mean F1                       0.684           0.665(nan)\n",
      "\n",
      " Target Precision              0.554           0.325(nan)\n",
      " Target Recall                 0.549           0.809(nan)\n",
      " Target F1                     0.551           0.464(nan)\n",
      "\n",
      " Orig Pred Precision           0.722           0.732(nan)\n",
      " Orig Pred Recall              0.668           0.696(nan)\n",
      " Orig Pred F1                  0.694           0.714(nan)\n",
      "\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " key-val (F)                   2.646           2.790(nan)\n",
      " val-keyN (F)                  2.829           2.881(nan)\n",
      " key-valN (F)                  2.872           2.834(nan)\n",
      " key-keyN (F)                  1.310           1.126(nan)\n",
      " val-valN (F)                  1.201           1.084(nan)\n",
      "\n",
      "Pre edit k -> v: [2.64561248] Post edit k -> v: [2.78994131]\n",
      "Metric: val-keyN (F)\n",
      "['cat-train-n02128925_4745/quickshift_masked_3']\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Accuracy                      0.687           0.654(nan)\n",
      " Mean Precision                0.692           0.731(nan)\n",
      " Mean Recall                   0.687           0.655(nan)\n",
      " Mean F1                       0.684           0.665(nan)\n",
      "\n",
      " Target Precision              0.554           0.325(nan)\n",
      " Target Recall                 0.549           0.809(nan)\n",
      " Target F1                     0.551           0.464(nan)\n",
      "\n",
      " Orig Pred Precision           0.722           0.732(nan)\n",
      " Orig Pred Recall              0.668           0.696(nan)\n",
      " Orig Pred F1                  0.694           0.714(nan)\n",
      "\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " key-val (F)                   2.646           2.790(nan)\n",
      " val-keyN (F)                  2.829           2.881(nan)\n",
      " key-valN (F)                  2.872           2.834(nan)\n",
      " key-keyN (F)                  1.310           1.126(nan)\n",
      " val-valN (F)                  1.201           1.084(nan)\n",
      "\n",
      "Pre edit k -> v: [2.64561248] Post edit k -> v: [2.78994131]\n",
      "Metric: key-valN (F)\n",
      "[]\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Accuracy                      nan             nan(nan)\n",
      " Mean Precision                nan             nan(nan)\n",
      " Mean Recall                   nan             nan(nan)\n",
      " Mean F1                       nan             nan(nan)\n",
      "\n",
      " Target Precision              nan             nan(nan)\n",
      " Target Recall                 nan             nan(nan)\n",
      " Target F1                     nan             nan(nan)\n",
      "\n",
      " Orig Pred Precision           nan             nan(nan)\n",
      " Orig Pred Recall              nan             nan(nan)\n",
      " Orig Pred F1                  nan             nan(nan)\n",
      "\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " key-val (F)                   nan             nan(nan)\n",
      " val-keyN (F)                  nan             nan(nan)\n",
      " key-valN (F)                  nan             nan(nan)\n",
      " key-keyN (F)                  nan             nan(nan)\n",
      " val-valN (F)                  nan             nan(nan)\n",
      "\n",
      "Pre edit k -> v: [] Post edit k -> v: []\n",
      "Metric: key-keyN (F)\n",
      "['cat-train-n02129165_3350/slic_gaussian_0'\n",
      " 'cat-train-n02129165_3350/slic_masked_1'\n",
      " 'cat-train-n02129165_3350/watershed_gaussian_3'\n",
      " 'cat-train-n02129604_11283/quickshift_masked_1'\n",
      " 'cat-train-n02129604_11283/slic_masked_2'\n",
      " 'cat-train-n02123597_8088/slic_masked_1']\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Accuracy                      0.687           0.672(0.018)\n",
      " Mean Precision                0.692           0.692(0.035)\n",
      " Mean Recall                   0.687           0.672(0.018)\n",
      " Mean F1                       0.684           0.669(0.026)\n",
      "\n",
      " Target Precision              0.554           0.406(0.048)\n",
      " Target Recall                 0.549           0.687(0.027)\n",
      " Target F1                     0.551           0.508(0.034)\n",
      "\n",
      " Orig Pred Precision           0.596           0.667(0.331)\n",
      " Orig Pred Recall              0.699           0.449(0.233)\n",
      " Orig Pred F1                  0.642           0.532(0.265)\n",
      "\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " key-val (F)                   4.031           2.493(0.204)\n",
      " val-keyN (F)                  4.160           2.685(0.182)\n",
      " key-valN (F)                  4.133           2.504(0.177)\n",
      " key-keyN (F)                  0.469           0.490(0.226)\n",
      " val-valN (F)                  0.871           0.792(0.076)\n",
      "\n",
      "Pre edit k -> v: [2.74486256 3.90970302 4.15272856 3.90263057 6.2078557  3.26895094] Post edit k -> v: [2.35481739 2.67474985 2.33262515 2.81797647 2.431777   2.34659076]\n",
      "Metric: val-valN (F)\n",
      "['cat-train-n02120997_13308/slic_masked_5'\n",
      " 'cat-train-n02125311_6892/watershed_masked_1'\n",
      " 'cat-train-n02123597_3714/slic_masked_2'\n",
      " 'cat-train-n02123597_3714/watershed_masked_6'\n",
      " 'cat-train-n02124313_1518/felzenszwalb_masked_4']\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Accuracy                      0.687           0.577(0.101)\n",
      " Mean Precision                0.692           0.637(0.023)\n",
      " Mean Recall                   0.687           0.577(0.101)\n",
      " Mean F1                       0.684           0.559(0.103)\n",
      "\n",
      " Target Precision              0.554           0.292(0.095)\n",
      " Target Recall                 0.549           0.795(0.109)\n",
      " Target F1                     0.551           0.413(0.093)\n",
      "\n",
      " Orig Pred Precision           0.668           0.174(0.389)\n",
      " Orig Pred Recall              0.701           0.022(0.049)\n",
      " Orig Pred F1                  0.682           0.039(0.087)\n",
      "\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " key-val (F)                   5.504           2.499(0.861)\n",
      " val-keyN (F)                  5.642           2.588(0.786)\n",
      " key-valN (F)                  5.525           2.547(0.800)\n",
      " key-keyN (F)                  0.760           0.671(0.390)\n",
      " val-valN (F)                  0.502           0.518(0.429)\n",
      "\n",
      "Pre edit k -> v: [6.23412752 6.91055536 5.78709173 5.79694414 2.79374814] Post edit k -> v: [4.01143169 2.39587569 1.99339986 2.01079321 2.08472204]\n"
     ]
    }
   ],
   "source": [
    "metrics = [\n",
    "    ['{} key-val (F)', '{} val-keyN (F)', '{} key-valN (F)', '{} key-keyN (F)', '{} val-valN (F)']\n",
    "]\n",
    "for metric in metrics[0]:\n",
    "    print(\"Metric:{}\".format(metric.format(\"\")))\n",
    "    kv_f = df[df[metric.format('Post')] > df[metric.format('Pre')]]\n",
    "    print(kv_f['ID'].to_numpy())\n",
    "    print_summary(kv_f)\n",
    "    print_summary(kv_f, metrics=metrics)\n",
    "    print(\"Pre edit k -> v: {} Post edit k -> v: {}\".format(\n",
    "        kv_f['Pre key-val (F)'].to_numpy(), kv_f['Post key-val (F)'].to_numpy()))\n",
    "\n",
    "\n",
    "# metrics = [\n",
    "#     ['{} key-val (L)', '{} val-keyN (L)', '{} key-valN (L)', '{} key-keyN (L)', '{} val-valN (L)']\n",
    "# ]\n",
    "# kv_f = df[df['Post key-val (L)'] > df['Pre key-val (L)']]\n",
    "# print(kv_f['ID'])\n",
    "# print_summary(kv_f)\n",
    "# print_summary(kv_f, metrics=metrics)\n",
    "# print(\"Pre edit k -> v: {} Post edit k -> v: {}\".format(kv_f['Post key-val (F)'].to_numpy(), kv_f['Pre key-val (F)'].to_numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 11: How do the edits that improve recall of both target and original class differ from those that only improve target class (and decrease original class recall)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = 'Recall'\n",
    "pos_target_pos_orig = df[ \n",
    "    (df['Post Target {}'.format(metric_name)] > df['Pre Target {}'.format(metric_name)]) &\n",
    "    (df['Post Orig Pred {}'.format(metric_name)] > df['Pre Orig Pred {}'.format(metric_name)])]  \n",
    "\n",
    "pos_target_neg_orig = df[ \n",
    "    (df['Post Target {}'.format(metric_name)] > df['Pre Target {}'.format(metric_name)]) &\n",
    "    (df['Post Orig Pred {}'.format(metric_name)] < df['Pre Orig Pred {}'.format(metric_name)])] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increase target and orig. prediction\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      "{} Accuracy                    0.687           0.673(0.010)\n",
      "{} Mean Precision              0.692           0.708(0.013)\n",
      "{} Mean Recall                 0.687           0.673(0.010)\n",
      "{} Mean F1                     0.684           0.675(0.006)\n",
      "\n",
      "{} Target Precision            0.554           0.430(0.068)\n",
      "{} Target Recall               0.549           0.684(0.094)\n",
      "{} Target F1                   0.551           0.519(0.027)\n",
      "\n",
      "{} Orig Pred Precision         0.703           0.696(0.052)\n",
      "{} Orig Pred Recall            0.626           0.648(0.104)\n",
      "{} Orig Pred F1                0.658           0.668(0.077)\n",
      "\n",
      "Increase target and decrease orig. prediction\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      "{} Accuracy                    0.687           0.640(0.047)\n",
      "{} Mean Precision              0.692           0.662(0.050)\n",
      "{} Mean Recall                 0.687           0.640(0.047)\n",
      "{} Mean F1                     0.684           0.628(0.056)\n",
      "\n",
      "{} Target Precision            0.554           0.383(0.086)\n",
      "{} Target Recall               0.549           0.695(0.079)\n",
      "{} Target F1                   0.551           0.482(0.059)\n",
      "\n",
      "{} Orig Pred Precision         0.679           0.458(0.416)\n",
      "{} Orig Pred Recall            0.632           0.232(0.253)\n",
      "{} Orig Pred F1                0.645           0.285(0.298)\n",
      "\n",
      "Pos Target Pos Orig:\n",
      "Key features \tmean: 56.429 median: 52.0\n",
      "Key logits \tmean: 56.714 median: 49.0\n",
      "Val features \tmean: 22.000 median: 20.0\n",
      "Val logits \tmean: 25.571 median: 22.0\n",
      "Pos Target Neg Orig: \n",
      "Key features \tmean: 81.925 median: 98.5\n",
      "Key logits \tmean: 82.375 median: 99.5\n",
      "Val features \tmean: 6.308 median: 0.0\n",
      "Val logits \tmean: 6.617 median: 0.0\n"
     ]
    }
   ],
   "source": [
    "def get_unique(df_):\n",
    "    ids = df_['ID']\n",
    "    print(ids.iloc[0])\n",
    "    unique_key_images = {}\n",
    "    unique_segmentation_methods = {}\n",
    "    unique_modification_methods = {}\n",
    "    for id_ in ids:\n",
    "        split_id = id_.split('/')\n",
    "        key_id = split_id[0]\n",
    "        if key_id in unique_key_images:\n",
    "            unique_key_images[key_id] += 1\n",
    "        else: \n",
    "            unique_key_images[key_id] = 1\n",
    "        \n",
    "        val_id = split_id[1].split('_')\n",
    "        segmentation_method = val_id[0]\n",
    "        if segmentation_method in unique_segmentation_methods:\n",
    "            unique_segmentation_methods[segmentation_method] += 1\n",
    "        else:\n",
    "            unique_segmentation_methods[segmentation_method] = 1\n",
    "        \n",
    "        modification_method = val_id[1]\n",
    "        if modification_method in unique_modification_methods:\n",
    "            unique_modification_methods[modification_method] += 1\n",
    "        else:\n",
    "            unique_modification_methods[modification_method] = 1\n",
    "    print(unique_key_images)\n",
    "    print(unique_segmentation_methods)\n",
    "    print(unique_modification_methods)\n",
    "    \n",
    "        \n",
    "print(\"Increase target and orig. prediction\")\n",
    "print_summary(pos_target_pos_orig)\n",
    "\n",
    "    \n",
    "# get_unique(pos_target_pos_orig)\n",
    "\n",
    "print(\"Increase target and decrease orig. prediction\")\n",
    "print_summary(pos_target_neg_orig)\n",
    "# get_unique(pos_target_neg_orig)\n",
    "\n",
    "# Print how many neighbors became target\n",
    "keys = [\n",
    "    \"Num of key's Neighbors Became Target (F)\", \n",
    "    # \"Num of key's Neighbors Unaffected (F)\", \n",
    "    \"Num of key's Neighbors Became Target (L)\",\n",
    "    # \"Num of key's Neighbors Unaffected (L)\", \n",
    "    \"Num of val's Neighbors Became Target (F)\", \n",
    "    # \"Num of val's Neighbors Unaffected (F)\", \n",
    "    \"Num of val's Neighbors Became Target (L)\",\n",
    "    # \"Num of val's Neighbors Unaffected (L)\", \n",
    "]\n",
    "key_abbr = [\n",
    "    \"Key features\", \"Key logits\", \"Val features\", \"Val logits\"\n",
    "]\n",
    "print(\"Pos Target Pos Orig:\")\n",
    "for key, abbr in zip(keys, key_abbr):\n",
    "    pp_key = pos_target_pos_orig[key]\n",
    "    # print(\"Key: {}\".format(key))\n",
    "    print(\"{} \\tmean: {:.3f} median: {}\".format(abbr, pp_key.mean(), pp_key.median()))\n",
    "    # print(pp_key)\n",
    "    \n",
    "print(\"Pos Target Neg Orig: \")\n",
    "for key, abbr in zip(keys, key_abbr):\n",
    "    pn_key = pos_target_neg_orig[key]\n",
    "    print(\"{} \\tmean: {:.3f} median: {}\".format(abbr, pn_key.mean(), pn_key.median()))\n",
    "    # print(pn_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 12: Given the changes in neighbor predictions, will we be able to see patterns in performance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 edits with 0-33 of key's neighbors becoming target\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Accuracy                      0.687           0.681(0.005)\n",
      " Mean Precision                0.692           0.698(0.004)\n",
      " Mean Recall                   0.687           0.681(0.005)\n",
      " Mean F1                       0.684           0.677(0.007)\n",
      "\n",
      " Target Precision              0.554           0.495(0.039)\n",
      " Target Recall                 0.549           0.601(0.038)\n",
      " Target F1                     0.551           0.541(0.012)\n",
      "\n",
      " Orig Pred Precision           0.694           0.783(0.061)\n",
      " Orig Pred Recall              0.583           0.499(0.179)\n",
      " Orig Pred F1                  0.627           0.590(0.136)\n",
      "\n",
      "26 edits with 34-66 of key's neighbors becoming target\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Accuracy                      0.687           0.673(0.019)\n",
      " Mean Precision                0.692           0.691(0.027)\n",
      " Mean Recall                   0.687           0.673(0.019)\n",
      " Mean F1                       0.684           0.668(0.027)\n",
      "\n",
      " Target Precision              0.554           0.467(0.047)\n",
      " Target Recall                 0.549           0.633(0.054)\n",
      " Target F1                     0.551           0.534(0.020)\n",
      "\n",
      " Orig Pred Precision           0.679           0.682(0.222)\n",
      " Orig Pred Recall              0.580           0.453(0.224)\n",
      " Orig Pred F1                  0.617           0.512(0.221)\n",
      "\n",
      "93 edits with 67-100 of key's neighbors becoming target\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Accuracy                      0.687           0.630(0.048)\n",
      " Mean Precision                0.692           0.654(0.053)\n",
      " Mean Recall                   0.687           0.630(0.048)\n",
      " Mean F1                       0.684           0.616(0.056)\n",
      "\n",
      " Target Precision              0.554           0.354(0.074)\n",
      " Target Recall                 0.549           0.719(0.074)\n",
      " Target F1                     0.551           0.466(0.057)\n",
      "\n",
      " Orig Pred Precision           0.681           0.387(0.433)\n",
      " Orig Pred Recall              0.652           0.180(0.242)\n",
      " Orig Pred F1                  0.657           0.226(0.294)\n",
      "\n",
      "114 edits with 0-25 of val's neighbors becoming target\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Accuracy                      0.687           0.637(0.047)\n",
      " Mean Precision                0.692           0.661(0.051)\n",
      " Mean Recall                   0.687           0.637(0.047)\n",
      " Mean F1                       0.684           0.624(0.055)\n",
      "\n",
      " Target Precision              0.554           0.376(0.085)\n",
      " Target Recall                 0.549           0.700(0.081)\n",
      " Target F1                     0.551           0.478(0.058)\n",
      "\n",
      " Orig Pred Precision           0.685           0.447(0.425)\n",
      " Orig Pred Recall              0.636           0.220(0.255)\n",
      " Orig Pred F1                  0.649           0.271(0.300)\n",
      "\n",
      "12 edits with 26-50 of val's neighbors becoming target\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Accuracy                      0.687           0.682(0.006)\n",
      " Mean Precision                0.692           0.699(0.005)\n",
      " Mean Recall                   0.687           0.682(0.006)\n",
      " Mean F1                       0.684           0.681(0.005)\n",
      "\n",
      " Target Precision              0.554           0.474(0.037)\n",
      " Target Recall                 0.549           0.635(0.043)\n",
      " Target F1                     0.551           0.540(0.013)\n",
      "\n",
      " Orig Pred Precision           0.655           0.709(0.052)\n",
      " Orig Pred Recall              0.611           0.562(0.104)\n",
      " Orig Pred F1                  0.626           0.623(0.076)\n",
      "\n",
      "2 edits with 51-75 of val's neighbors becoming target\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Accuracy                      0.687           0.686(0.002)\n",
      " Mean Precision                0.692           0.700(0.005)\n",
      " Mean Recall                   0.687           0.686(0.002)\n",
      " Mean F1                       0.684           0.684(0.000)\n",
      "\n",
      " Target Precision              0.554           0.469(0.032)\n",
      " Target Recall                 0.549           0.655(0.041)\n",
      " Target F1                     0.551           0.545(0.007)\n",
      "\n",
      " Orig Pred Precision           0.600           0.611(0.023)\n",
      " Orig Pred Recall              0.610           0.595(0.022)\n",
      " Orig Pred F1                  0.605           0.602(0.000)\n",
      "\n",
      "0 edits with 76-100 of val's neighbors becoming target\n",
      "Metric                         Pre-Edit        Post-Edit           \n",
      " Accuracy                      nan             nan(nan)\n",
      " Mean Precision                nan             nan(nan)\n",
      " Mean Recall                   nan             nan(nan)\n",
      " Mean F1                       nan             nan(nan)\n",
      "\n",
      " Target Precision              nan             nan(nan)\n",
      " Target Recall                 nan             nan(nan)\n",
      " Target F1                     nan             nan(nan)\n",
      "\n",
      " Orig Pred Precision           nan             nan(nan)\n",
      " Orig Pred Recall              nan             nan(nan)\n",
      " Orig Pred F1                  nan             nan(nan)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thirds = [(0, 33), (34,66), (67, 100)]\n",
    "quarters = [(0, 25), (26, 50), (51, 75), (76, 100)]\n",
    "for (low, high) in thirds:\n",
    "    cur_third = df[((df[\"Num of key's Neighbors Became Target (F)\"] >= low) &\n",
    "                    (df[\"Num of key's Neighbors Became Target (F)\"] <= high))]\n",
    "\n",
    "    print(\"{} edits with {}-{} of key's neighbors becoming target\".format(len(cur_third), low, high))\n",
    "    print_summary(cur_third)\n",
    "    \n",
    "for (low, high) in quarters:\n",
    "    cur_third = df[((df[\"Num of val's Neighbors Became Target (F)\"] >= low) &\n",
    "                    (df[\"Num of val's Neighbors Became Target (F)\"] <= high))]\n",
    "\n",
    "    print(\"{} edits with {}-{} of val's neighbors becoming target\".format(len(cur_third), low, high))\n",
    "    print_summary(cur_third)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "editing",
   "language": "python",
   "name": "editing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
